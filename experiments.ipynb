{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitting as sp\n",
    "import train\n",
    "from pruning_utils.prune import *\n",
    "from pruning_utils.pruners import *\n",
    "from pruning_utils.generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-tier MON, CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 10\n",
      "alpha: 0.5\t iters: 9\n",
      "alpha: 0.25\t iters: 12\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.26 [12928/50000 (26%)]\tLoss: 1.8413\tError: 65.62\n",
      "Fwd iters: 8.56\tFwd Time: 0.0957\tBkwd Iters: 8.68\tBkwd Time: 0.1391\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 11\n",
      "alpha: 0.5\t iters: 9\n",
      "alpha: 0.25\t iters: 12\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.51 [25728/50000 (51%)]\tLoss: 1.6348\tError: 59.38\n",
      "Fwd iters: 9.00\tFwd Time: 0.0979\tBkwd Iters: 8.00\tBkwd Time: 0.1323\n",
      "\n",
      "Train Epoch: 0.77 [38528/50000 (77%)]\tLoss: 1.3809\tError: 46.09\n",
      "Fwd iters: 9.47\tFwd Time: 0.1019\tBkwd Iters: 8.03\tBkwd Time: 0.1292\n",
      "\n",
      "Tot train time: 106.92895436286926\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.3093, Error: 4600/10000 (46.00%)\n",
      "Tot test time: 8.091797351837158\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=False)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.MultiConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        conv_sizes=(16,32,60),\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-2,\n",
    "        lr_mode='step',\n",
    "        step=10,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single convolution MON, CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fft_rfft() got an unexpected keyword argument 'onesided'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fe1bd12c800c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLoader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcifar_loaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m train.train(trainLoader, testLoader,\n\u001b[0m\u001b[0;32m      4\u001b[0m       train.SingleConvNet(sp.MONPeacemanRachford,\n\u001b[0;32m      5\u001b[0m                         \u001b[0min_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainLoader, testLoader, model, epochs, max_lr, print_freq, change_mo, model_path, lr_mode, step, tune_alpha, max_alpha)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mce_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\splitting.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Run the forward pass _without_ tracking gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             z = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_inverse\u001b[1;34m(self, alpha, beta)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mAfft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_fft_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[0mBfft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_fft_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         I = torch.eye(Afft.shape[1], dtype=Afft.dtype,\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_fft_conv\u001b[1;34m(weight, hw)\u001b[0m\n\u001b[0;32m    136\u001b[0m     kernel = F.pad(F.pad(kernel, (0, hw[0] - weight.shape[2], 0, hw[1] - weight.shape[3])),\n\u001b[0;32m    137\u001b[0m                    (0, py, 0, px), mode=\"circular\")[:, :, py:, px:]\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfft_to_complex_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monesided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fft_rfft() got an unexpected keyword argument 'onesided'"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=False)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.SingleConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        out_channels=81,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-3,\n",
    "        lr_mode='step',\n",
    "        step=25,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-tier MON, CIFAR-10 + data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 7\n",
      "alpha: 0.25\t iters: 12\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.26 [12928/50000 (26%)]\tLoss: 1.9840\tError: 75.00\n",
      "Fwd iters: 7.70\tFwd Time: 0.3544\tBkwd Iters: 7.99\tBkwd Time: 0.4853\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 7\n",
      "alpha: 0.25\t iters: 11\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.51 [25728/50000 (51%)]\tLoss: 1.8562\tError: 67.97\n",
      "Fwd iters: 7.00\tFwd Time: 0.3321\tBkwd Iters: 8.00\tBkwd Time: 0.4866\n",
      "\n",
      "Train Epoch: 0.77 [38528/50000 (77%)]\tLoss: 1.7976\tError: 60.94\n",
      "Fwd iters: 7.81\tFwd Time: 0.3585\tBkwd Iters: 8.00\tBkwd Time: 0.4870\n",
      "\n",
      "Tot train time: 358.17819261550903\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.5719, Error: 5797/10000 (57.97%)\n",
      "Tot test time: 23.59424901008606\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=True)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.MultiConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        conv_sizes=(64,128,128),\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=0.05,\n",
    "        lr_mode='1cycle',\n",
    "        change_mo=True,\n",
    "#         epochs=65,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single convolution MON, CIFAR-10 + data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 8\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.26 [12928/50000 (26%)]\tLoss: 1.4396\tError: 50.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.4187\tBkwd Iters: 6.00\tBkwd Time: 0.4716\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 8\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.51 [25728/50000 (51%)]\tLoss: 1.4426\tError: 49.22\n",
      "Fwd iters: 6.00\tFwd Time: 0.4179\tBkwd Iters: 6.00\tBkwd Time: 0.4705\n",
      "\n",
      "Train Epoch: 0.77 [38528/50000 (77%)]\tLoss: 1.4308\tError: 51.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.4195\tBkwd Iters: 6.00\tBkwd Time: 0.4716\n",
      "\n",
      "Tot train time: 383.9007320404053\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.2711, Error: 4491/10000 (44.91%)\n",
      "Tot test time: 25.07926917076111\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=True)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.SingleConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        out_channels=200,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-2,\n",
    "        lr_mode='1cycle',\n",
    "        change_mo=True,\n",
    "#         epochs=65,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-tier MON, SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/train_32x32.mat\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 8\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.17 [12928/73257 (17%)]\tLoss: 2.2158\tError: 78.91\n",
      "Fwd iters: 6.99\tFwd Time: 0.0829\tBkwd Iters: 7.32\tBkwd Time: 0.1166\n",
      "\n",
      "Train Epoch: 0.35 [25728/73257 (35%)]\tLoss: 1.8257\tError: 64.84\n",
      "Fwd iters: 8.17\tFwd Time: 0.0918\tBkwd Iters: 10.15\tBkwd Time: 0.1582\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 9\n",
      "alpha: 0.5\t iters: 8\n",
      "alpha: 0.25\t iters: 12\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.52 [38528/73257 (52%)]\tLoss: 1.2114\tError: 36.72\n",
      "Fwd iters: 8.00\tFwd Time: 0.0945\tBkwd Iters: 8.00\tBkwd Time: 0.1257\n",
      "\n",
      "Train Epoch: 0.70 [51328/73257 (70%)]\tLoss: 0.8822\tError: 22.66\n",
      "Fwd iters: 8.00\tFwd Time: 0.0920\tBkwd Iters: 8.00\tBkwd Time: 0.1258\n",
      "\n",
      "Train Epoch: 0.87 [64128/73257 (87%)]\tLoss: 0.9077\tError: 23.44\n",
      "Fwd iters: 8.00\tFwd Time: 0.0910\tBkwd Iters: 8.00\tBkwd Time: 0.1253\n",
      "\n",
      "Tot train time: 151.86186623573303\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7197, Error: 5396/26032 (20.73%)\n",
      "Tot test time: 19.842275619506836\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.svhn_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.MultiConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        conv_sizes=(16,32,60),\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-3,\n",
    "        lr_mode='step',\n",
    "        step=10,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single convolution MON, SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/train_32x32.mat\n",
      "Using downloaded and verified file: data/test_32x32.mat\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 5\n",
      "alpha: 0.5\t iters: 5\n",
      "alpha: 0.25\t iters: 8\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.17 [12928/73257 (17%)]\tLoss: 1.6229\tError: 43.75\n",
      "Fwd iters: 5.18\tFwd Time: 0.0974\tBkwd Iters: 6.00\tBkwd Time: 0.1792\n",
      "\n",
      "Train Epoch: 0.35 [25728/73257 (35%)]\tLoss: 0.9959\tError: 29.69\n",
      "Fwd iters: 6.00\tFwd Time: 0.1071\tBkwd Iters: 6.00\tBkwd Time: 0.1767\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 5\n",
      "alpha: 0.5\t iters: 6\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.52 [38528/73257 (52%)]\tLoss: 0.9007\tError: 28.91\n",
      "Fwd iters: 5.13\tFwd Time: 0.0966\tBkwd Iters: 5.00\tBkwd Time: 0.1495\n",
      "\n",
      "Train Epoch: 0.70 [51328/73257 (70%)]\tLoss: 0.7787\tError: 24.22\n",
      "Fwd iters: 5.75\tFwd Time: 0.1035\tBkwd Iters: 5.14\tBkwd Time: 0.1533\n",
      "\n",
      "Train Epoch: 0.87 [64128/73257 (87%)]\tLoss: 0.7090\tError: 19.53\n",
      "Fwd iters: 6.00\tFwd Time: 0.1058\tBkwd Iters: 5.95\tBkwd Time: 0.1751\n",
      "\n",
      "Tot train time: 178.84369277954102\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7594, Error: 5671/26032 (21.78%)\n",
      "Tot test time: 21.099106550216675\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.svhn_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.SingleConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        out_channels=81,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-3,\n",
    "        lr_mode='step',\n",
    "        step=25,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-tier MON, MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 9\n",
      "alpha: 0.5\t iters: 7\n",
      "alpha: 0.25\t iters: 11\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.21 [12928/60000 (21%)]\tLoss: 0.2957\tError: 8.59\n",
      "Fwd iters: 7.00\tFwd Time: 0.0668\tBkwd Iters: 7.03\tBkwd Time: 0.0880\n",
      "\n",
      "Train Epoch: 0.43 [25728/60000 (43%)]\tLoss: 0.2262\tError: 6.25\n",
      "Fwd iters: 7.00\tFwd Time: 0.0686\tBkwd Iters: 7.97\tBkwd Time: 0.1009\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 9\n",
      "alpha: 0.5\t iters: 7\n",
      "alpha: 0.25\t iters: 11\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.64 [38528/60000 (64%)]\tLoss: 0.0849\tError: 0.78\n",
      "Fwd iters: 7.00\tFwd Time: 0.0666\tBkwd Iters: 8.00\tBkwd Time: 0.1000\n",
      "\n",
      "Train Epoch: 0.85 [51328/60000 (85%)]\tLoss: 0.2064\tError: 5.47\n",
      "Fwd iters: 7.00\tFwd Time: 0.0698\tBkwd Iters: 8.00\tBkwd Time: 0.1030\n",
      "\n",
      "Tot train time: 95.81319689750671\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1206, Error: 369/10000 (3.69%)\n",
      "Tot test time: 4.875991582870483\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.MultiConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28,\n",
    "                        in_channels=1,\n",
    "                        conv_sizes=(16,32,32),\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-3,\n",
    "        lr_mode='step',\n",
    "        step=10,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single convolution MON, MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 5\n",
      "alpha: 0.5\t iters: 6\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.21 [12928/60000 (21%)]\tLoss: 0.2081\tError: 4.69\n",
      "Fwd iters: 5.00\tFwd Time: 0.0431\tBkwd Iters: 5.00\tBkwd Time: 0.0763\n",
      "\n",
      "Train Epoch: 0.43 [25728/60000 (43%)]\tLoss: 0.1635\tError: 3.12\n",
      "Fwd iters: 5.00\tFwd Time: 0.0419\tBkwd Iters: 5.06\tBkwd Time: 0.0767\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 5\n",
      "alpha: 0.5\t iters: 6\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.64 [38528/60000 (64%)]\tLoss: 0.2346\tError: 6.25\n",
      "Fwd iters: 5.00\tFwd Time: 0.0418\tBkwd Iters: 5.97\tBkwd Time: 0.0893\n",
      "\n",
      "Train Epoch: 0.85 [51328/60000 (85%)]\tLoss: 0.1615\tError: 7.03\n",
      "Fwd iters: 5.00\tFwd Time: 0.0418\tBkwd Iters: 6.00\tBkwd Time: 0.0898\n",
      "\n",
      "Tot train time: 72.66634154319763\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0938, Error: 256/10000 (2.56%)\n",
      "Tot test time: 4.002958536148071\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.SingleConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28,\n",
    "                        in_channels=1,\n",
    "                        out_channels=54,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-3,\n",
    "        lr_mode='step',\n",
    "        step=10,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single fully-connected MON, MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.21 [12928/60000 (21%)]\tLoss: 0.3294\tError: 12.50\n",
      "Fwd iters: 6.00\tFwd Time: 0.0127\tBkwd Iters: 6.00\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 0.43 [25728/60000 (43%)]\tLoss: 0.4375\tError: 10.94\n",
      "Fwd iters: 6.00\tFwd Time: 0.0123\tBkwd Iters: 6.00\tBkwd Time: 0.0060\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.64 [38528/60000 (64%)]\tLoss: 0.2073\tError: 7.03\n",
      "Fwd iters: 6.00\tFwd Time: 0.0119\tBkwd Iters: 6.00\tBkwd Time: 0.0060\n",
      "\n",
      "Train Epoch: 0.85 [51328/60000 (85%)]\tLoss: 0.1452\tError: 3.91\n",
      "Fwd iters: 6.00\tFwd Time: 0.0123\tBkwd Iters: 6.00\tBkwd Time: 0.0060\n",
      "\n",
      "Tot train time: 23.067858457565308\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1840, Error: 557/10000 (5.57%)\n",
      "Tot test time: 2.1772100925445557\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "      train.SingleFcNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28**2,\n",
    "                        out_dim=87,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0),\n",
    "        max_lr=1e-3,\n",
    "        lr_mode='step',\n",
    "        step=10,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked single fully-connected MonDEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvh16\\anaconda3\\envs\\pvh\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking checked\n",
      "Globally masking checked\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 11\n",
      "alpha: 0.5\t iters: 8\n",
      "norm of U.weight is \t 19.363445281982422\n",
      "norm of U.bias is \t 0.6480432748794556\n",
      "norm of A.weight is \t 19.0712833404541\n",
      "norm of B.weight is \t 18.834001541137695\n",
      "alpha: 0.25\t iters: 9\n",
      "norm of U.weight is \t 19.363445281982422\n",
      "norm of U.bias is \t 0.6480432748794556\n",
      "norm of A.weight is \t 19.0712833404541\n",
      "norm of B.weight is \t 18.834001541137695\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.21 [12928/60000 (21%)]\tLoss: 0.2956\tError: 10.16\n",
      "Fwd iters: 8.00\tFwd Time: 0.0167\tBkwd Iters: 6.17\tBkwd Time: 0.0051\n",
      "\n",
      "Train Epoch: 0.43 [25728/60000 (43%)]\tLoss: 0.1190\tError: 3.91\n",
      "Fwd iters: 8.00\tFwd Time: 0.0180\tBkwd Iters: 6.90\tBkwd Time: 0.0056\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 12\n",
      "alpha: 0.5\t iters: 8\n",
      "norm of U.weight is \t 22.003938674926758\n",
      "norm of U.bias is \t 0.6545919179916382\n",
      "norm of A.weight is \t 20.790420532226562\n",
      "norm of B.weight is \t 19.83503532409668\n",
      "alpha: 0.25\t iters: 9\n",
      "norm of U.weight is \t 22.003938674926758\n",
      "norm of U.bias is \t 0.6545919179916382\n",
      "norm of A.weight is \t 20.790420532226562\n",
      "norm of B.weight is \t 19.83503532409668\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.64 [38528/60000 (64%)]\tLoss: 0.0983\tError: 2.34\n",
      "Fwd iters: 8.00\tFwd Time: 0.0167\tBkwd Iters: 7.00\tBkwd Time: 0.0056\n",
      "\n",
      "Train Epoch: 0.85 [51328/60000 (85%)]\tLoss: 0.1255\tError: 1.56\n",
      "Fwd iters: 8.00\tFwd Time: 0.0178\tBkwd Iters: 7.01\tBkwd Time: 0.0059\n",
      "\n",
      "Tot train time: 22.049251317977905\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1049, Error: 326/10000 (3.26%)\n",
      "Tot test time: 1.5946192741394043\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 13\n",
      "alpha: 0.5\t iters: 8\n",
      "norm of U.weight is \t 24.676362991333008\n",
      "norm of U.bias is \t 0.6629622578620911\n",
      "norm of A.weight is \t 23.425334930419922\n",
      "norm of B.weight is \t 21.223499298095703\n",
      "alpha: 0.25\t iters: 9\n",
      "norm of U.weight is \t 24.676362991333008\n",
      "norm of U.bias is \t 0.6629622578620911\n",
      "norm of A.weight is \t 23.425334930419922\n",
      "norm of B.weight is \t 21.223499298095703\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 1.21 [12928/60000 (21%)]\tLoss: 0.0453\tError: 0.78\n",
      "Fwd iters: 8.54\tFwd Time: 0.0172\tBkwd Iters: 7.07\tBkwd Time: 0.0058\n",
      "\n",
      "Train Epoch: 1.43 [25728/60000 (43%)]\tLoss: 0.0885\tError: 3.12\n",
      "Fwd iters: 9.00\tFwd Time: 0.0183\tBkwd Iters: 7.13\tBkwd Time: 0.0059\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 14\n",
      "alpha: 0.5\t iters: 9\n",
      "norm of U.weight is \t 26.340078353881836\n",
      "norm of U.bias is \t 0.6755572557449341\n",
      "norm of A.weight is \t 25.145671844482422\n",
      "norm of B.weight is \t 22.1387882232666\n",
      "alpha: 0.25\t iters: 9\n",
      "norm of U.weight is \t 26.340078353881836\n",
      "norm of U.bias is \t 0.6755572557449341\n",
      "norm of A.weight is \t 25.145671844482422\n",
      "norm of B.weight is \t 22.1387882232666\n",
      "alpha: 0.125\t iters: 12\n",
      "norm of U.weight is \t 26.340078353881836\n",
      "norm of U.bias is \t 0.6755572557449341\n",
      "norm of A.weight is \t 25.145671844482422\n",
      "norm of B.weight is \t 22.1387882232666\n",
      "setting to:  0.25\n",
      "--------------\n",
      "\n",
      "Train Epoch: 1.64 [38528/60000 (64%)]\tLoss: 0.0528\tError: 3.12\n",
      "Fwd iters: 9.00\tFwd Time: 0.0185\tBkwd Iters: 9.00\tBkwd Time: 0.0074\n",
      "\n",
      "Train Epoch: 1.85 [51328/60000 (85%)]\tLoss: 0.0755\tError: 1.56\n",
      "Fwd iters: 9.15\tFwd Time: 0.0185\tBkwd Iters: 8.99\tBkwd Time: 0.0072\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fecb40ad87f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m train.train(trainLoader, testLoader,\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mmax_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainLoader, testLoader, model, epochs, max_lr, print_freq, change_mo, model_path, lr_mode, step, tune_alpha, max_alpha)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mce_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\splitting.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Run the forward pass _without_ tracking gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             z = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_inverse\u001b[1;34m(self, alpha, beta)\u001b[0m\n\u001b[0;32m     91\u001b[0m                       device=self.A.get_masked_weight().device)\n\u001b[0;32m     92\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_masked_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_masked_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_masked_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_masked_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "model = train.SingleFcNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28**2,\n",
    "                        out_dim=1000,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = Mag(masked_parameters_)\n",
    "\n",
    "compression = 0\n",
    "\n",
    "# args.compression = 10**(arg.compression)\n",
    "sparsity = 10**(-float(compression))\n",
    "prune_loop(model, None, pruner, None, 'cpu', sparsity, schedule='exponential', scope='global', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "            model,\n",
    "            max_lr=1e-3,\n",
    "            lr_mode='step',\n",
    "            step=10,\n",
    "            change_mo=False,\n",
    "            epochs=40,\n",
    "        #     epochs=1,\n",
    "            print_freq=100,\n",
    "            tune_alpha=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 8\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.21 [12928/60000 (21%)]\tLoss: 0.3567\tError: 11.72\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.00\tBkwd Time: 0.0055\n",
      "\n",
      "Train Epoch: 0.43 [25728/60000 (43%)]\tLoss: 0.3839\tError: 10.16\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0056\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.64 [38528/60000 (64%)]\tLoss: 0.3831\tError: 9.38\n",
      "Fwd iters: 6.00\tFwd Time: 0.0058\tBkwd Iters: 6.00\tBkwd Time: 0.0053\n",
      "\n",
      "Train Epoch: 0.85 [51328/60000 (85%)]\tLoss: 0.1500\tError: 2.34\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0052\n",
      "\n",
      "Tot train time: 13.726603031158447\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2042, Error: 603/10000 (6.03%)\n",
      "Tot test time: 1.2849042415618896\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 1.21 [12928/60000 (21%)]\tLoss: 0.2649\tError: 6.25\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.00\tBkwd Time: 0.0055\n",
      "\n",
      "Train Epoch: 1.43 [25728/60000 (43%)]\tLoss: 0.2056\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0051\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 6\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 1.64 [38528/60000 (64%)]\tLoss: 0.0775\tError: 2.34\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.00\tBkwd Time: 0.0060\n",
      "\n",
      "Train Epoch: 1.85 [51328/60000 (85%)]\tLoss: 0.1900\tError: 7.81\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.00\tBkwd Time: 0.0065\n",
      "\n",
      "Tot train time: 13.74596619606018\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1456, Error: 445/10000 (4.45%)\n",
      "Tot test time: 1.2855803966522217\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 2.21 [12928/60000 (21%)]\tLoss: 0.1461\tError: 3.91\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0053\n",
      "\n",
      "Train Epoch: 2.43 [25728/60000 (43%)]\tLoss: 0.0904\tError: 2.34\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0059\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 2.64 [38528/60000 (64%)]\tLoss: 0.0807\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0065\tBkwd Iters: 6.00\tBkwd Time: 0.0053\n",
      "\n",
      "Train Epoch: 2.85 [51328/60000 (85%)]\tLoss: 0.1648\tError: 3.91\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0054\n",
      "\n",
      "Tot train time: 13.575585842132568\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1172, Error: 353/10000 (3.53%)\n",
      "Tot test time: 1.314413070678711\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 3.21 [12928/60000 (21%)]\tLoss: 0.0406\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.00\tBkwd Time: 0.0052\n",
      "\n",
      "Train Epoch: 3.43 [25728/60000 (43%)]\tLoss: 0.1031\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.00\tBkwd Time: 0.0057\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 3.64 [38528/60000 (64%)]\tLoss: 0.0923\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.00\tBkwd Time: 0.0050\n",
      "\n",
      "Train Epoch: 3.85 [51328/60000 (85%)]\tLoss: 0.0804\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.01\tBkwd Time: 0.0056\n",
      "\n",
      "Tot train time: 13.607227802276611\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1080, Error: 333/10000 (3.33%)\n",
      "Tot test time: 1.2896416187286377\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 4.21 [12928/60000 (21%)]\tLoss: 0.1793\tError: 6.25\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.03\tBkwd Time: 0.0055\n",
      "\n",
      "Train Epoch: 4.43 [25728/60000 (43%)]\tLoss: 0.0575\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.03\tBkwd Time: 0.0060\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 4.64 [38528/60000 (64%)]\tLoss: 0.1066\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.06\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 4.85 [51328/60000 (85%)]\tLoss: 0.0934\tError: 3.91\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.07\tBkwd Time: 0.0061\n",
      "\n",
      "Tot train time: 13.816589117050171\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0923, Error: 292/10000 (2.92%)\n",
      "Tot test time: 1.2903602123260498\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 5.21 [12928/60000 (21%)]\tLoss: 0.0374\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.11\tBkwd Time: 0.0057\n",
      "\n",
      "Train Epoch: 5.43 [25728/60000 (43%)]\tLoss: 0.0706\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.10\tBkwd Time: 0.0053\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 7\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 5.64 [38528/60000 (64%)]\tLoss: 0.0459\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.21\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 5.85 [51328/60000 (85%)]\tLoss: 0.1303\tError: 3.91\n",
      "Fwd iters: 6.00\tFwd Time: 0.0059\tBkwd Iters: 6.12\tBkwd Time: 0.0053\n",
      "\n",
      "Tot train time: 13.660027742385864\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0912, Error: 279/10000 (2.79%)\n",
      "Tot test time: 1.2895944118499756\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 6.21 [12928/60000 (21%)]\tLoss: 0.0229\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0059\tBkwd Iters: 6.25\tBkwd Time: 0.0060\n",
      "\n",
      "Train Epoch: 6.43 [25728/60000 (43%)]\tLoss: 0.0884\tError: 2.34\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.24\tBkwd Time: 0.0055\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 6.64 [38528/60000 (64%)]\tLoss: 0.0319\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.34\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 6.85 [51328/60000 (85%)]\tLoss: 0.0869\tError: 2.34\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.38\tBkwd Time: 0.0055\n",
      "\n",
      "Tot train time: 13.870813131332397\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0791, Error: 248/10000 (2.48%)\n",
      "Tot test time: 1.316706657409668\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 7.21 [12928/60000 (21%)]\tLoss: 0.0713\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.55\tBkwd Time: 0.0063\n",
      "\n",
      "Train Epoch: 7.43 [25728/60000 (43%)]\tLoss: 0.0437\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.50\tBkwd Time: 0.0056\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 7.64 [38528/60000 (64%)]\tLoss: 0.0906\tError: 4.69\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.64\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 7.85 [51328/60000 (85%)]\tLoss: 0.0332\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.60\tBkwd Time: 0.0056\n",
      "\n",
      "Tot train time: 13.862220048904419\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0879, Error: 284/10000 (2.84%)\n",
      "Tot test time: 1.2855587005615234\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 8.21 [12928/60000 (21%)]\tLoss: 0.0373\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0064\tBkwd Iters: 6.79\tBkwd Time: 0.0056\n",
      "\n",
      "Train Epoch: 8.43 [25728/60000 (43%)]\tLoss: 0.0465\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.80\tBkwd Time: 0.0067\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 8.64 [38528/60000 (64%)]\tLoss: 0.0411\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.78\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 8.85 [51328/60000 (85%)]\tLoss: 0.0670\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.75\tBkwd Time: 0.0063\n",
      "\n",
      "Tot train time: 14.060413122177124\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0817, Error: 256/10000 (2.56%)\n",
      "Tot test time: 1.2868132591247559\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 9.21 [12928/60000 (21%)]\tLoss: 0.0153\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.77\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 9.43 [25728/60000 (43%)]\tLoss: 0.0643\tError: 3.12\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.83\tBkwd Time: 0.0068\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 9.64 [38528/60000 (64%)]\tLoss: 0.0786\tError: 3.12\n",
      "Fwd iters: 6.03\tFwd Time: 0.0057\tBkwd Iters: 6.91\tBkwd Time: 0.0061\n",
      "\n",
      "Train Epoch: 9.85 [51328/60000 (85%)]\tLoss: 0.0116\tError: 0.00\n",
      "Fwd iters: 6.02\tFwd Time: 0.0063\tBkwd Iters: 6.85\tBkwd Time: 0.0059\n",
      "\n",
      "Tot train time: 14.05852723121643\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0731, Error: 226/10000 (2.26%)\n",
      "Tot test time: 1.2927114963531494\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 10.21 [12928/60000 (21%)]\tLoss: 0.0365\tError: 1.56\n",
      "Fwd iters: 6.10\tFwd Time: 0.0062\tBkwd Iters: 6.97\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 10.43 [25728/60000 (43%)]\tLoss: 0.0193\tError: 0.00\n",
      "Fwd iters: 6.16\tFwd Time: 0.0062\tBkwd Iters: 6.92\tBkwd Time: 0.0061\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 10.64 [38528/60000 (64%)]\tLoss: 0.0121\tError: 0.00\n",
      "Fwd iters: 6.10\tFwd Time: 0.0064\tBkwd Iters: 6.90\tBkwd Time: 0.0061\n",
      "\n",
      "Train Epoch: 10.85 [51328/60000 (85%)]\tLoss: 0.0323\tError: 0.78\n",
      "Fwd iters: 6.12\tFwd Time: 0.0063\tBkwd Iters: 6.90\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 14.003877878189087\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0657, Error: 218/10000 (2.18%)\n",
      "Tot test time: 1.320523977279663\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 11.21 [12928/60000 (21%)]\tLoss: 0.0433\tError: 1.56\n",
      "Fwd iters: 6.08\tFwd Time: 0.0062\tBkwd Iters: 6.94\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 11.43 [25728/60000 (43%)]\tLoss: 0.0141\tError: 0.00\n",
      "Fwd iters: 6.06\tFwd Time: 0.0061\tBkwd Iters: 6.89\tBkwd Time: 0.0061\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 11.64 [38528/60000 (64%)]\tLoss: 0.0243\tError: 0.00\n",
      "Fwd iters: 6.03\tFwd Time: 0.0059\tBkwd Iters: 6.97\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 11.85 [51328/60000 (85%)]\tLoss: 0.0113\tError: 0.00\n",
      "Fwd iters: 6.05\tFwd Time: 0.0061\tBkwd Iters: 6.95\tBkwd Time: 0.0061\n",
      "\n",
      "Tot train time: 13.970278978347778\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 222/10000 (2.22%)\n",
      "Tot test time: 1.2864763736724854\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 12.21 [12928/60000 (21%)]\tLoss: 0.0565\tError: 1.56\n",
      "Fwd iters: 6.04\tFwd Time: 0.0061\tBkwd Iters: 6.92\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 12.43 [25728/60000 (43%)]\tLoss: 0.0119\tError: 0.00\n",
      "Fwd iters: 6.06\tFwd Time: 0.0063\tBkwd Iters: 6.93\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 12.64 [38528/60000 (64%)]\tLoss: 0.0104\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0058\tBkwd Iters: 6.94\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 12.85 [51328/60000 (85%)]\tLoss: 0.0167\tError: 0.78\n",
      "Fwd iters: 6.03\tFwd Time: 0.0061\tBkwd Iters: 6.94\tBkwd Time: 0.0059\n",
      "\n",
      "Tot train time: 14.066468238830566\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0641, Error: 219/10000 (2.19%)\n",
      "Tot test time: 1.309722900390625\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 13.21 [12928/60000 (21%)]\tLoss: 0.0195\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0059\tBkwd Iters: 6.93\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 13.43 [25728/60000 (43%)]\tLoss: 0.0666\tError: 2.34\n",
      "Fwd iters: 6.02\tFwd Time: 0.0060\tBkwd Iters: 6.91\tBkwd Time: 0.0060\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 13.64 [38528/60000 (64%)]\tLoss: 0.0158\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0059\tBkwd Iters: 6.93\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 13.85 [51328/60000 (85%)]\tLoss: 0.0627\tError: 1.56\n",
      "Fwd iters: 6.04\tFwd Time: 0.0060\tBkwd Iters: 6.94\tBkwd Time: 0.0070\n",
      "\n",
      "Tot train time: 13.94627571105957\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0642, Error: 214/10000 (2.14%)\n",
      "Tot test time: 1.2929012775421143\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 14.21 [12928/60000 (21%)]\tLoss: 0.0218\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.93\tBkwd Time: 0.0060\n",
      "\n",
      "Train Epoch: 14.43 [25728/60000 (43%)]\tLoss: 0.0092\tError: 0.00\n",
      "Fwd iters: 6.02\tFwd Time: 0.0061\tBkwd Iters: 6.96\tBkwd Time: 0.0062\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 14.64 [38528/60000 (64%)]\tLoss: 0.0155\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.93\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 14.85 [51328/60000 (85%)]\tLoss: 0.0107\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0060\tBkwd Iters: 6.93\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 13.991088628768921\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0647, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.29355788230896\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 15.21 [12928/60000 (21%)]\tLoss: 0.0103\tError: 0.00\n",
      "Fwd iters: 6.06\tFwd Time: 0.0061\tBkwd Iters: 6.93\tBkwd Time: 0.0068\n",
      "\n",
      "Train Epoch: 15.43 [25728/60000 (43%)]\tLoss: 0.0034\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.96\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 7\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 15.64 [38528/60000 (64%)]\tLoss: 0.0484\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0059\tBkwd Iters: 6.96\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 15.85 [51328/60000 (85%)]\tLoss: 0.0254\tError: 0.78\n",
      "Fwd iters: 6.02\tFwd Time: 0.0062\tBkwd Iters: 6.92\tBkwd Time: 0.0063\n",
      "\n",
      "Tot train time: 13.978203535079956\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0652, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.2936005592346191\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 16.21 [12928/60000 (21%)]\tLoss: 0.0219\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0062\tBkwd Iters: 6.92\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 16.43 [25728/60000 (43%)]\tLoss: 0.0211\tError: 0.78\n",
      "Fwd iters: 6.02\tFwd Time: 0.0064\tBkwd Iters: 6.96\tBkwd Time: 0.0062\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 16.64 [38528/60000 (64%)]\tLoss: 0.0060\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.94\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 16.85 [51328/60000 (85%)]\tLoss: 0.0106\tError: 0.00\n",
      "Fwd iters: 6.02\tFwd Time: 0.0060\tBkwd Iters: 6.96\tBkwd Time: 0.0063\n",
      "\n",
      "Tot train time: 13.989172220230103\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 212/10000 (2.12%)\n",
      "Tot test time: 1.2894442081451416\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 17.21 [12928/60000 (21%)]\tLoss: 0.0458\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.90\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 17.43 [25728/60000 (43%)]\tLoss: 0.0214\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.96\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 17.64 [38528/60000 (64%)]\tLoss: 0.0296\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0064\tBkwd Iters: 6.91\tBkwd Time: 0.0058\n",
      "\n",
      "Train Epoch: 17.85 [51328/60000 (85%)]\tLoss: 0.0413\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0060\tBkwd Iters: 6.96\tBkwd Time: 0.0063\n",
      "\n",
      "Tot train time: 13.92176365852356\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0653, Error: 212/10000 (2.12%)\n",
      "Tot test time: 1.2910635471343994\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 18.21 [12928/60000 (21%)]\tLoss: 0.0073\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0076\tBkwd Iters: 6.97\tBkwd Time: 0.0063\n",
      "\n",
      "Train Epoch: 18.43 [25728/60000 (43%)]\tLoss: 0.0097\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0073\tBkwd Iters: 6.90\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 18.64 [38528/60000 (64%)]\tLoss: 0.0218\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0078\tBkwd Iters: 6.91\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 18.85 [51328/60000 (85%)]\tLoss: 0.0299\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0074\tBkwd Iters: 6.88\tBkwd Time: 0.0063\n",
      "\n",
      "Tot train time: 14.927298069000244\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0654, Error: 218/10000 (2.18%)\n",
      "Tot test time: 1.321427345275879\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 19.21 [12928/60000 (21%)]\tLoss: 0.0133\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0072\tBkwd Iters: 6.93\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 19.43 [25728/60000 (43%)]\tLoss: 0.0048\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0076\tBkwd Iters: 6.89\tBkwd Time: 0.0066\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 19.64 [38528/60000 (64%)]\tLoss: 0.0289\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0073\tBkwd Iters: 6.94\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 19.85 [51328/60000 (85%)]\tLoss: 0.0177\tError: 0.00\n",
      "Fwd iters: 6.02\tFwd Time: 0.0076\tBkwd Iters: 6.94\tBkwd Time: 0.0065\n",
      "\n",
      "Tot train time: 15.049543142318726\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0657, Error: 215/10000 (2.15%)\n",
      "Tot test time: 1.387937307357788\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 20.21 [12928/60000 (21%)]\tLoss: 0.0124\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0074\tBkwd Iters: 6.92\tBkwd Time: 0.0068\n",
      "\n",
      "Train Epoch: 20.43 [25728/60000 (43%)]\tLoss: 0.0275\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0074\tBkwd Iters: 6.91\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 20.64 [38528/60000 (64%)]\tLoss: 0.0203\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0073\tBkwd Iters: 6.97\tBkwd Time: 0.0068\n",
      "\n",
      "Train Epoch: 20.85 [51328/60000 (85%)]\tLoss: 0.0118\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0076\tBkwd Iters: 6.96\tBkwd Time: 0.0066\n",
      "\n",
      "Tot train time: 15.083374738693237\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0648, Error: 212/10000 (2.12%)\n",
      "Tot test time: 1.308577299118042\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 21.21 [12928/60000 (21%)]\tLoss: 0.0175\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0077\tBkwd Iters: 6.96\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 21.43 [25728/60000 (43%)]\tLoss: 0.0214\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0074\tBkwd Iters: 6.94\tBkwd Time: 0.0065\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 21.64 [38528/60000 (64%)]\tLoss: 0.0242\tError: 0.78\n",
      "Fwd iters: 6.04\tFwd Time: 0.0078\tBkwd Iters: 6.99\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 21.85 [51328/60000 (85%)]\tLoss: 0.0186\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0080\tBkwd Iters: 6.89\tBkwd Time: 0.0061\n",
      "\n",
      "Tot train time: 15.263857364654541\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0649, Error: 211/10000 (2.11%)\n",
      "Tot test time: 1.2789254188537598\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 22.21 [12928/60000 (21%)]\tLoss: 0.0176\tError: 0.00\n",
      "Fwd iters: 6.03\tFwd Time: 0.0074\tBkwd Iters: 6.92\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 22.43 [25728/60000 (43%)]\tLoss: 0.0095\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0075\tBkwd Iters: 6.94\tBkwd Time: 0.0065\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 22.64 [38528/60000 (64%)]\tLoss: 0.0208\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0077\tBkwd Iters: 6.97\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 22.85 [51328/60000 (85%)]\tLoss: 0.0184\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0074\tBkwd Iters: 6.91\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 15.058329343795776\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0649, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.4018898010253906\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 23.21 [12928/60000 (21%)]\tLoss: 0.0069\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0078\tBkwd Iters: 6.89\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 23.43 [25728/60000 (43%)]\tLoss: 0.0231\tError: 0.00\n",
      "Fwd iters: 6.02\tFwd Time: 0.0070\tBkwd Iters: 6.95\tBkwd Time: 0.0065\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 23.64 [38528/60000 (64%)]\tLoss: 0.0153\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0076\tBkwd Iters: 6.93\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 23.85 [51328/60000 (85%)]\tLoss: 0.0090\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0071\tBkwd Iters: 6.96\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 14.923671960830688\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0650, Error: 213/10000 (2.13%)\n",
      "Tot test time: 1.3659346103668213\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 24.21 [12928/60000 (21%)]\tLoss: 0.0147\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0073\tBkwd Iters: 6.92\tBkwd Time: 0.0065\n",
      "\n",
      "Train Epoch: 24.43 [25728/60000 (43%)]\tLoss: 0.0093\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0077\tBkwd Iters: 6.91\tBkwd Time: 0.0065\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 24.64 [38528/60000 (64%)]\tLoss: 0.0082\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0072\tBkwd Iters: 6.91\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 24.85 [51328/60000 (85%)]\tLoss: 0.0183\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0077\tBkwd Iters: 6.94\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 14.982112884521484\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0650, Error: 213/10000 (2.13%)\n",
      "Tot test time: 1.3781545162200928\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 25.21 [12928/60000 (21%)]\tLoss: 0.0100\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0075\tBkwd Iters: 6.96\tBkwd Time: 0.0063\n",
      "\n",
      "Train Epoch: 25.43 [25728/60000 (43%)]\tLoss: 0.0072\tError: 0.00\n",
      "Fwd iters: 6.02\tFwd Time: 0.0072\tBkwd Iters: 6.90\tBkwd Time: 0.0068\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 25.64 [38528/60000 (64%)]\tLoss: 0.0174\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0066\tBkwd Iters: 6.96\tBkwd Time: 0.0069\n",
      "\n",
      "Train Epoch: 25.85 [51328/60000 (85%)]\tLoss: 0.0108\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0064\tBkwd Iters: 6.92\tBkwd Time: 0.0059\n",
      "\n",
      "Tot train time: 14.57514238357544\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0650, Error: 211/10000 (2.11%)\n",
      "Tot test time: 1.2952401638031006\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 26.21 [12928/60000 (21%)]\tLoss: 0.0151\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.94\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 26.43 [25728/60000 (43%)]\tLoss: 0.0078\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.89\tBkwd Time: 0.0061\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 26.64 [38528/60000 (64%)]\tLoss: 0.0450\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.91\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 26.85 [51328/60000 (85%)]\tLoss: 0.0090\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.96\tBkwd Time: 0.0061\n",
      "\n",
      "Tot train time: 13.995688676834106\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0652, Error: 212/10000 (2.12%)\n",
      "Tot test time: 1.2995491027832031\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 27.21 [12928/60000 (21%)]\tLoss: 0.0060\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0066\tBkwd Iters: 6.92\tBkwd Time: 0.0067\n",
      "\n",
      "Train Epoch: 27.43 [25728/60000 (43%)]\tLoss: 0.0059\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.89\tBkwd Time: 0.0062\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 27.64 [38528/60000 (64%)]\tLoss: 0.0092\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0058\tBkwd Iters: 6.94\tBkwd Time: 0.0072\n",
      "\n",
      "Train Epoch: 27.85 [51328/60000 (85%)]\tLoss: 0.0470\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.96\tBkwd Time: 0.0067\n",
      "\n",
      "Tot train time: 14.215315341949463\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0652, Error: 213/10000 (2.13%)\n",
      "Tot test time: 1.2935564517974854\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 28.21 [12928/60000 (21%)]\tLoss: 0.0288\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0065\tBkwd Iters: 6.92\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 28.43 [25728/60000 (43%)]\tLoss: 0.0067\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0062\tBkwd Iters: 6.91\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 28.64 [38528/60000 (64%)]\tLoss: 0.0161\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0064\tBkwd Iters: 6.88\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 28.85 [51328/60000 (85%)]\tLoss: 0.0140\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0064\tBkwd Iters: 6.90\tBkwd Time: 0.0066\n",
      "\n",
      "Tot train time: 14.267221689224243\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 211/10000 (2.11%)\n",
      "Tot test time: 1.293440341949463\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 29.21 [12928/60000 (21%)]\tLoss: 0.0081\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.94\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 29.43 [25728/60000 (43%)]\tLoss: 0.0115\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.95\tBkwd Time: 0.0067\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 29.64 [38528/60000 (64%)]\tLoss: 0.0043\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0063\tBkwd Iters: 6.90\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 29.85 [51328/60000 (85%)]\tLoss: 0.0107\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.91\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 14.298013687133789\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0650, Error: 211/10000 (2.11%)\n",
      "Tot test time: 1.2875206470489502\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 30.21 [12928/60000 (21%)]\tLoss: 0.0099\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0067\tBkwd Iters: 6.93\tBkwd Time: 0.0064\n",
      "\n",
      "Train Epoch: 30.43 [25728/60000 (43%)]\tLoss: 0.0216\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.90\tBkwd Time: 0.0061\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 30.64 [38528/60000 (64%)]\tLoss: 0.0089\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.99\tBkwd Time: 0.0073\n",
      "\n",
      "Train Epoch: 30.85 [51328/60000 (85%)]\tLoss: 0.0063\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.93\tBkwd Time: 0.0073\n",
      "\n",
      "Tot train time: 14.335956335067749\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.2985999584197998\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 31.21 [12928/60000 (21%)]\tLoss: 0.0209\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0066\tBkwd Iters: 6.93\tBkwd Time: 0.0073\n",
      "\n",
      "Train Epoch: 31.43 [25728/60000 (43%)]\tLoss: 0.0220\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0064\tBkwd Iters: 6.90\tBkwd Time: 0.0065\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 31.64 [38528/60000 (64%)]\tLoss: 0.0096\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0067\tBkwd Iters: 6.96\tBkwd Time: 0.0060\n",
      "\n",
      "Train Epoch: 31.85 [51328/60000 (85%)]\tLoss: 0.0097\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0061\tBkwd Iters: 6.93\tBkwd Time: 0.0062\n",
      "\n",
      "Tot train time: 14.324424266815186\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.2976047992706299\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 32.21 [12928/60000 (21%)]\tLoss: 0.0086\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0066\tBkwd Iters: 6.93\tBkwd Time: 0.0063\n",
      "\n",
      "Train Epoch: 32.43 [25728/60000 (43%)]\tLoss: 0.0251\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.96\tBkwd Time: 0.0066\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 32.64 [38528/60000 (64%)]\tLoss: 0.0143\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.85\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 32.85 [51328/60000 (85%)]\tLoss: 0.0094\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.93\tBkwd Time: 0.0064\n",
      "\n",
      "Tot train time: 14.27083134651184\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.2897069454193115\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 33.21 [12928/60000 (21%)]\tLoss: 0.0204\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.93\tBkwd Time: 0.0060\n",
      "\n",
      "Train Epoch: 33.43 [25728/60000 (43%)]\tLoss: 0.0072\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.97\tBkwd Time: 0.0060\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 33.64 [38528/60000 (64%)]\tLoss: 0.0108\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.91\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 33.85 [51328/60000 (85%)]\tLoss: 0.0139\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.95\tBkwd Time: 0.0069\n",
      "\n",
      "Tot train time: 14.03669285774231\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.320450782775879\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 34.21 [12928/60000 (21%)]\tLoss: 0.1064\tError: 1.56\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.94\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 34.43 [25728/60000 (43%)]\tLoss: 0.0120\tError: 0.78\n",
      "Fwd iters: 6.01\tFwd Time: 0.0061\tBkwd Iters: 6.88\tBkwd Time: 0.0063\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 34.64 [38528/60000 (64%)]\tLoss: 0.0108\tError: 0.00\n",
      "Fwd iters: 6.04\tFwd Time: 0.0062\tBkwd Iters: 6.91\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 34.85 [51328/60000 (85%)]\tLoss: 0.0093\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0066\tBkwd Iters: 6.92\tBkwd Time: 0.0061\n",
      "\n",
      "Tot train time: 14.160711526870728\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.3076221942901611\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 35.21 [12928/60000 (21%)]\tLoss: 0.0087\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0059\tBkwd Iters: 6.99\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 35.43 [25728/60000 (43%)]\tLoss: 0.0063\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.95\tBkwd Time: 0.0065\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 35.64 [38528/60000 (64%)]\tLoss: 0.0241\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0065\tBkwd Iters: 6.99\tBkwd Time: 0.0062\n",
      "\n",
      "Train Epoch: 35.85 [51328/60000 (85%)]\tLoss: 0.0063\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.95\tBkwd Time: 0.0068\n",
      "\n",
      "Tot train time: 14.28150224685669\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.2943086624145508\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 36.21 [12928/60000 (21%)]\tLoss: 0.0132\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0056\tBkwd Iters: 6.90\tBkwd Time: 0.0068\n",
      "\n",
      "Train Epoch: 36.43 [25728/60000 (43%)]\tLoss: 0.0279\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.95\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 36.64 [38528/60000 (64%)]\tLoss: 0.0037\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.88\tBkwd Time: 0.0059\n",
      "\n",
      "Train Epoch: 36.85 [51328/60000 (85%)]\tLoss: 0.0165\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.95\tBkwd Time: 0.0070\n",
      "\n",
      "Tot train time: 14.301140308380127\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.2936277389526367\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 37.21 [12928/60000 (21%)]\tLoss: 0.0176\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0057\tBkwd Iters: 6.93\tBkwd Time: 0.0056\n",
      "\n",
      "Train Epoch: 37.43 [25728/60000 (43%)]\tLoss: 0.0044\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0063\tBkwd Iters: 6.88\tBkwd Time: 0.0058\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 37.64 [38528/60000 (64%)]\tLoss: 0.0078\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.91\tBkwd Time: 0.0066\n",
      "\n",
      "Train Epoch: 37.85 [51328/60000 (85%)]\tLoss: 0.0139\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0061\tBkwd Iters: 6.93\tBkwd Time: 0.0065\n",
      "\n",
      "Tot train time: 13.994044780731201\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 211/10000 (2.11%)\n",
      "Tot test time: 1.2926795482635498\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 38.21 [12928/60000 (21%)]\tLoss: 0.0134\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0062\tBkwd Iters: 6.96\tBkwd Time: 0.0067\n",
      "\n",
      "Train Epoch: 38.43 [25728/60000 (43%)]\tLoss: 0.0081\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.93\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 38.64 [38528/60000 (64%)]\tLoss: 0.0064\tError: 0.00\n",
      "Fwd iters: 6.00\tFwd Time: 0.0060\tBkwd Iters: 6.96\tBkwd Time: 0.0069\n",
      "\n",
      "Train Epoch: 38.85 [51328/60000 (85%)]\tLoss: 0.0065\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0065\tBkwd Iters: 6.92\tBkwd Time: 0.0062\n",
      "\n",
      "Tot train time: 14.113579273223877\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 210/10000 (2.10%)\n",
      "Tot test time: 1.3169519901275635\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 39.21 [12928/60000 (21%)]\tLoss: 0.0098\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0059\tBkwd Iters: 6.92\tBkwd Time: 0.0070\n",
      "\n",
      "Train Epoch: 39.43 [25728/60000 (43%)]\tLoss: 0.0121\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0062\tBkwd Iters: 6.89\tBkwd Time: 0.0064\n",
      "\n",
      "----tuning alpha----\n",
      "current:  0.5\n",
      "alpha: 1.0\t iters: 8\n",
      "alpha: 0.5\t iters: 6\n",
      "alpha: 0.25\t iters: 9\n",
      "setting to:  0.5\n",
      "--------------\n",
      "\n",
      "Train Epoch: 39.64 [38528/60000 (64%)]\tLoss: 0.0297\tError: 0.78\n",
      "Fwd iters: 6.00\tFwd Time: 0.0057\tBkwd Iters: 6.96\tBkwd Time: 0.0068\n",
      "\n",
      "Train Epoch: 39.85 [51328/60000 (85%)]\tLoss: 0.0126\tError: 0.00\n",
      "Fwd iters: 6.01\tFwd Time: 0.0061\tBkwd Iters: 6.91\tBkwd Time: 0.0077\n",
      "\n",
      "Tot train time: 14.21530795097351\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Error: 212/10000 (2.12%)\n",
      "Tot test time: 1.2966523170471191\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "model = train.SingleFcNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28**2,\n",
    "                        out_dim=87,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = Mag(masked_parameters_)\n",
    "\n",
    "compression = 0.25\n",
    "\n",
    "# args.compression = 10**(arg.compression)\n",
    "sparsity = 10**(-float(compression))\n",
    "prune_loop(model, None, pruner, None, 'cpu', sparsity, schedule='exponential', scope='global', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "            model,\n",
    "            max_lr=1e-3,\n",
    "            lr_mode='step',\n",
    "            step=10,\n",
    "            change_mo=False,\n",
    "            epochs=40,\n",
    "        #     epochs=1,\n",
    "            print_freq=100,\n",
    "            tune_alpha=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking checked\n",
      "Globally masking checked\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 1.4409581422805786\n",
      "norm of U.bias is \t 0.03734884783625603\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.21 [12928/60000 (21%)]\tLoss: 1.3162\tError: 41.41\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0019\n",
      "\n",
      "Train Epoch: 0.43 [25728/60000 (43%)]\tLoss: 0.6778\tError: 15.62\n",
      "Fwd iters: 2.00\tFwd Time: 0.0026\tBkwd Iters: 2.00\tBkwd Time: 0.0020\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 3.151210069656372\n",
      "norm of U.bias is \t 0.0812150165438652\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 0.64 [38528/60000 (64%)]\tLoss: 0.6011\tError: 16.41\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 0.85 [51328/60000 (85%)]\tLoss: 0.6817\tError: 17.97\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Tot train time: 10.569763898849487\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6188, Error: 1807/10000 (18.07%)\n",
      "Tot test time: 1.2934150695800781\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 3.9893834590911865\n",
      "norm of U.bias is \t 0.0876610204577446\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 1.21 [12928/60000 (21%)]\tLoss: 0.6438\tError: 17.19\n",
      "Fwd iters: 2.00\tFwd Time: 0.0023\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 1.43 [25728/60000 (43%)]\tLoss: 0.7337\tError: 21.09\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 4.3438029289245605\n",
      "norm of U.bias is \t 0.08644886314868927\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 1.64 [38528/60000 (64%)]\tLoss: 0.4720\tError: 15.62\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 1.85 [51328/60000 (85%)]\tLoss: 0.6013\tError: 18.75\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "Tot train time: 10.570894241333008\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5156, Error: 1517/10000 (15.17%)\n",
      "Tot test time: 1.2562899589538574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 4.6912946701049805\n",
      "norm of U.bias is \t 0.08603356033563614\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 2.21 [12928/60000 (21%)]\tLoss: 0.5640\tError: 22.66\n",
      "Fwd iters: 2.00\tFwd Time: 0.0026\tBkwd Iters: 2.00\tBkwd Time: 0.0024\n",
      "\n",
      "Train Epoch: 2.43 [25728/60000 (43%)]\tLoss: 0.3606\tError: 10.94\n",
      "Fwd iters: 2.00\tFwd Time: 0.0026\tBkwd Iters: 2.00\tBkwd Time: 0.0023\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 4.896751880645752\n",
      "norm of U.bias is \t 0.0834386944770813\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 2.64 [38528/60000 (64%)]\tLoss: 0.6622\tError: 17.19\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0024\n",
      "\n",
      "Train Epoch: 2.85 [51328/60000 (85%)]\tLoss: 0.3972\tError: 8.59\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0020\n",
      "\n",
      "Tot train time: 10.940247774124146\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4777, Error: 1420/10000 (14.20%)\n",
      "Tot test time: 1.2102782726287842\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 5.150904655456543\n",
      "norm of U.bias is \t 0.08401818573474884\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 3.21 [12928/60000 (21%)]\tLoss: 0.5180\tError: 16.41\n",
      "Fwd iters: 2.00\tFwd Time: 0.0027\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "Train Epoch: 3.43 [25728/60000 (43%)]\tLoss: 0.5162\tError: 15.62\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 5.316458702087402\n",
      "norm of U.bias is \t 0.08210457116365433\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 3.64 [38528/60000 (64%)]\tLoss: 0.4859\tError: 14.06\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 3.85 [51328/60000 (85%)]\tLoss: 0.4043\tError: 11.72\n",
      "Fwd iters: 2.00\tFwd Time: 0.0026\tBkwd Iters: 2.00\tBkwd Time: 0.0023\n",
      "\n",
      "Tot train time: 10.762455224990845\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4577, Error: 1347/10000 (13.47%)\n",
      "Tot test time: 1.3802659511566162\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 5.508144378662109\n",
      "norm of U.bias is \t 0.08477648347616196\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 4.21 [12928/60000 (21%)]\tLoss: 0.4814\tError: 15.62\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0023\n",
      "\n",
      "Train Epoch: 4.43 [25728/60000 (43%)]\tLoss: 0.4427\tError: 17.19\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0023\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 5.656948566436768\n",
      "norm of U.bias is \t 0.08581867814064026\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 4.64 [38528/60000 (64%)]\tLoss: 0.4816\tError: 14.84\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "Train Epoch: 4.85 [51328/60000 (85%)]\tLoss: 0.3968\tError: 14.84\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0020\n",
      "\n",
      "Tot train time: 10.85790228843689\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4555, Error: 1353/10000 (13.53%)\n",
      "Tot test time: 1.231797695159912\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 5.838200569152832\n",
      "norm of U.bias is \t 0.08913719654083252\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 5.21 [12928/60000 (21%)]\tLoss: 0.3628\tError: 8.59\n",
      "Fwd iters: 2.00\tFwd Time: 0.0023\tBkwd Iters: 2.00\tBkwd Time: 0.0020\n",
      "\n",
      "Train Epoch: 5.43 [25728/60000 (43%)]\tLoss: 0.2751\tError: 5.47\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0023\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 5.932117938995361\n",
      "norm of U.bias is \t 0.09075981378555298\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 5.64 [38528/60000 (64%)]\tLoss: 0.4420\tError: 14.06\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0024\n",
      "\n",
      "Train Epoch: 5.85 [51328/60000 (85%)]\tLoss: 0.3511\tError: 10.16\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Tot train time: 10.682461023330688\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4407, Error: 1324/10000 (13.24%)\n",
      "Tot test time: 1.243896484375\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.115818977355957\n",
      "norm of U.bias is \t 0.09159785509109497\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 6.21 [12928/60000 (21%)]\tLoss: 0.4557\tError: 10.16\n",
      "Fwd iters: 2.00\tFwd Time: 0.0026\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 6.43 [25728/60000 (43%)]\tLoss: 0.3417\tError: 10.94\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0020\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.224670886993408\n",
      "norm of U.bias is \t 0.09253458678722382\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 6.64 [38528/60000 (64%)]\tLoss: 0.5400\tError: 17.19\n",
      "Fwd iters: 2.00\tFwd Time: 0.0023\tBkwd Iters: 2.00\tBkwd Time: 0.0025\n",
      "\n",
      "Train Epoch: 6.85 [51328/60000 (85%)]\tLoss: 0.2951\tError: 9.38\n",
      "Fwd iters: 2.00\tFwd Time: 0.0022\tBkwd Iters: 2.00\tBkwd Time: 0.0020\n",
      "\n",
      "Tot train time: 10.681099891662598\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4375, Error: 1303/10000 (13.03%)\n",
      "Tot test time: 1.249791145324707\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.357511043548584\n",
      "norm of U.bias is \t 0.09579938650131226\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 7.21 [12928/60000 (21%)]\tLoss: 0.4037\tError: 14.84\n",
      "Fwd iters: 2.00\tFwd Time: 0.0023\tBkwd Iters: 2.00\tBkwd Time: 0.0023\n",
      "\n",
      "Train Epoch: 7.43 [25728/60000 (43%)]\tLoss: 0.4619\tError: 13.28\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.4795355796813965\n",
      "norm of U.bias is \t 0.09751784801483154\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 7.64 [38528/60000 (64%)]\tLoss: 0.3228\tError: 7.03\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 7.85 [51328/60000 (85%)]\tLoss: 0.5084\tError: 13.28\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "Tot train time: 10.70095443725586\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4372, Error: 1317/10000 (13.17%)\n",
      "Tot test time: 1.219785213470459\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.613327503204346\n",
      "norm of U.bias is \t 0.10239369422197342\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 8.21 [12928/60000 (21%)]\tLoss: 0.4019\tError: 11.72\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 8.43 [25728/60000 (43%)]\tLoss: 0.3194\tError: 10.16\n",
      "Fwd iters: 2.00\tFwd Time: 0.0027\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.69959831237793\n",
      "norm of U.bias is \t 0.10362668335437775\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 8.64 [38528/60000 (64%)]\tLoss: 0.5270\tError: 16.41\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Train Epoch: 8.85 [51328/60000 (85%)]\tLoss: 0.4505\tError: 15.62\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0019\n",
      "\n",
      "Tot train time: 10.733408451080322\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4318, Error: 1268/10000 (12.68%)\n",
      "Tot test time: 1.3252577781677246\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.826717376708984\n",
      "norm of U.bias is \t 0.10341249406337738\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 9.21 [12928/60000 (21%)]\tLoss: 0.4297\tError: 10.94\n",
      "Fwd iters: 2.00\tFwd Time: 0.0026\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "Train Epoch: 9.43 [25728/60000 (43%)]\tLoss: 0.4226\tError: 14.06\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0021\n",
      "\n",
      "----tuning alpha----\n",
      "current:  1.0\n",
      "alpha: 1.0\t iters: 2\n",
      "alpha: 0.5\t iters: 5\n",
      "norm of U.weight is \t 6.926016807556152\n",
      "norm of U.bias is \t 0.10675538331270218\n",
      "norm of A.weight is \t 1.1860395669937134\n",
      "norm of B.weight is \t 1.2026153802871704\n",
      "setting to:  1.0\n",
      "--------------\n",
      "\n",
      "Train Epoch: 9.64 [38528/60000 (64%)]\tLoss: 0.3272\tError: 10.16\n",
      "Fwd iters: 2.00\tFwd Time: 0.0025\tBkwd Iters: 2.00\tBkwd Time: 0.0024\n",
      "\n",
      "Train Epoch: 9.85 [51328/60000 (85%)]\tLoss: 0.4213\tError: 13.28\n",
      "Fwd iters: 2.00\tFwd Time: 0.0024\tBkwd Iters: 2.00\tBkwd Time: 0.0022\n",
      "\n",
      "Tot train time: 10.846486568450928\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.4340, Error: 1289/10000 (12.89%)\n",
      "Tot test time: 1.2231512069702148\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "# trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=False)\n",
    "\n",
    "model = train.SingleFcNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28**2,\n",
    "                        out_dim=5,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = Rand(masked_parameters_)\n",
    "\n",
    "# compression = 3\n",
    "\n",
    "# args.compression = 10**(arg.compression)\n",
    "# sparsity = 10**(-float(compression))\n",
    "sparsity = 1\n",
    "prune_loop(model, nn.CrossEntropyLoss(), pruner, trainLoader, 'cpu', sparsity, schedule='exponential', scope='global', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "with torch.no_grad():\n",
    "    # model.mon.linear_module.A.weight_mask = torch.zeros_like(model.mon.linear_module.A.weight_mask) + torch.eye(model.mon.linear_module.A.weight_mask.shape[0])\n",
    "    # model.mon.linear_module.B.weight_mask = torch.zeros_like(model.mon.linear_module.B.weight_mask) + torch.eye(model.mon.linear_module.B.weight_mask.shape[0])\n",
    "    model.mon.linear_module.A.weight_mask = torch.zeros_like(model.mon.linear_module.A.weight_mask)\n",
    "    model.mon.linear_module.B.weight_mask = torch.zeros_like(model.mon.linear_module.B.weight_mask)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "            model,\n",
    "            max_lr=1e-3,\n",
    "            lr_mode='step',\n",
    "            step=10,\n",
    "            change_mo=False,\n",
    "            # epochs=40,\n",
    "            epochs=10,\n",
    "            print_freq=100,\n",
    "            tune_alpha=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mon.linear_module.U.weight_mask\n",
      "mon.linear_module.U.bias_mask\n",
      "mon.linear_module.A.weight_mask\n",
      "mon.linear_module.B.weight_mask\n"
     ]
    }
   ],
   "source": [
    "mask = []\n",
    "with torch.no_grad():\n",
    "    for m, p in model.named_buffers():\n",
    "        print(m)\n",
    "        mask.append(p.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1584x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(22,4))\n",
    "fig_id = 0\n",
    "for i, (name, mask) in enumerate(model.named_buffers()):\n",
    "    if 'weight' in name:\n",
    "        fig_id += 1\n",
    "        ax = fig.add_subplot(1, 4, fig_id)\n",
    "        ax = sns.heatmap(mask.data.cpu())\n",
    "        ax.set_title(name)\n",
    "        ax.set_ylabel('neuron')\n",
    "        ax.set_xlabel('neuron')\n",
    "        # break\n",
    "# plt.savefig(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10232929922807542"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**-0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvh16\\anaconda3\\envs\\pvh\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONLeakyReLU\n",
      "sparsity is \t  0.683772233983162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 9.5360e-01,  4.4894e-01, -1.3972e-03,  ...,  5.2321e-01,\n",
      "          3.2526e-02, -2.9642e-03],\n",
      "        [ 5.2334e-01, -3.6914e-04, -1.4419e-02,  ...,  4.0771e-01,\n",
      "          2.6533e-02, -8.1759e-03],\n",
      "        [ 8.6392e-01, -3.5234e-03, -3.7098e-03,  ...,  6.9011e-02,\n",
      "          5.4052e-01,  4.9280e-01],\n",
      "        ...,\n",
      "        [ 2.9256e-01,  5.4894e-01, -9.0002e-03,  ...,  5.8832e-01,\n",
      "          2.7571e-01, -1.0619e-03],\n",
      "        [ 3.8346e-01,  4.2708e-01, -1.3432e-03,  ..., -2.0200e-03,\n",
      "          3.0932e-01,  1.6118e-01],\n",
      "        [ 1.0801e+00,  1.3852e-01, -2.7901e-03,  ..., -1.2355e-03,\n",
      "          4.1396e-01, -4.1432e-03]], grad_fn=<LeakyReluBackward0>),)\n",
      "Updated 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type __int64 but found float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f1feb7256885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msparsity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sparsity is \\t '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msparsity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m prune_loop(model, nn.CrossEntropyLoss(), pruner, trainLoader, 'cpu', sparsity, schedule='exponential', scope='local', epochs=1,\n\u001b[0m\u001b[0;32m     21\u001b[0m                reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\pruning_utils\\prune.py\u001b[0m in \u001b[0;36mprune_loop\u001b[1;34m(model, loss, pruner, dataloader, device, sparsity, schedule, scope, epochs, reinitialize, train_mode, shuffle, invert, store_mask, pruner_name, compression, dataset, args)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Prune model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mpruner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschedule\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'exponential'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparsity\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\pruning_utils\\pruners.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, model, loss, dataloader, device)\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_parameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0mflatten_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mstopped_grads\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mflatten_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# _forward_cls is defined by derived class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\splitting.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(ctx, *g)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonlin_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m             \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36mderivative\u001b[1;34m(self, *z)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Updated 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# z = torch.where(z>0, 1, 0.01)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Updated 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# z = torch.where(z>0, 1, 0.01)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type __int64 but found float"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "model = train.SingleFcNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28**2,\n",
    "                        out_dim=10,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = GraSP(masked_parameters_)\n",
    "\n",
    "compression = 0.5\n",
    "# args.compression = 10**(arg.compression)\n",
    "sparsity = 10**(-float(compression))\n",
    "print('sparsity is \\t ', str(1 - sparsity))\n",
    "prune_loop(model, nn.CrossEntropyLoss(), pruner, trainLoader, 'cpu', sparsity, schedule='exponential', scope='local', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "            model,\n",
    "            max_lr=1e-3,\n",
    "            lr_mode='step',\n",
    "            step=10,\n",
    "            change_mo=False,\n",
    "            epochs=10,\n",
    "        #     epochs=1,\n",
    "            print_freq=100,\n",
    "            tune_alpha=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3167, 0.1010, 0.8924, 0.8326],\n",
       "        [0.9371, 0.6876, 0.0982, 0.4602],\n",
       "        [0.9460, 0.1841, 0.0445, 0.7532]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand((3,4))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3167)\n",
      "tensor(0.1010)\n",
      "tensor(0.8924)\n",
      "tensor(0.8326)\n",
      "tensor(0.9371)\n",
      "tensor(0.6876)\n",
      "tensor(0.0982)\n",
      "tensor(0.4602)\n",
      "tensor(0.9460)\n",
      "tensor(0.1841)\n",
      "tensor(0.0445)\n",
      "tensor(0.7532)\n"
     ]
    }
   ],
   "source": [
    "for z_ in z:\n",
    "    for z__ in z_:\n",
    "        if z__ > 0:\n",
    "            print(z__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0] > 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9b6500d9a8c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-9b6500d9a8c7>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "tuple(torch.ones_like(z_).type_as(z[0]) if (z_>0) else torch.ones_like(z_)*0.01.type_as(z[0]) for z_ in z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9351.575009225146"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "83346*10**(-0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleFcNet(\n",
       "  (mon): MONPeacemanRachford(\n",
       "    (linear_module): MaskedMONSingleFc(\n",
       "      (U): Linear(in_features=784, out_features=87, bias=True)\n",
       "      (A): Linear(in_features=87, out_features=87, bias=False)\n",
       "      (B): Linear(in_features=87, out_features=87, bias=False)\n",
       "    )\n",
       "    (nonlin_module): MONReLU()\n",
       "  )\n",
       "  (Wout): Linear(in_features=87, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pvh16\\anaconda3\\envs\\pvh\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity is \t  0.2056717652757185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking checked\n",
      "Locally Masking checked\n",
      "threshold: \t  tensor(0.)\n",
      "max \t  tensor(0.0100)\n",
      "threshold: \t  tensor(0.)\n",
      "max \t  tensor(0.1065)\n",
      "threshold: \t  tensor(0.)\n",
      "max \t  tensor(0.0631)\n",
      "ERROR: 58779.0 prunable parameters remaining, expected 66204.08105132997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6e6f1fd78893>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0msparsity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sparsity is \\t '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msparsity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m prune_loop(model, nn.CrossEntropyLoss(), pruner, trainLoader, 'cpu', sparsity, schedule='exponential', scope='local', epochs=1,\n\u001b[0m\u001b[0;32m     21\u001b[0m                reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\pruning_utils\\prune.py\u001b[0m in \u001b[0;36mprune_loop\u001b[1;34m(model, loss, pruner, dataloader, device, sparsity, schedule, scope, epochs, reinitialize, train_mode, shuffle, invert, store_mask, pruner_name, compression, dataset, args)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining_params\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtotal_params\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msparsity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ERROR: {} prunable parameters remaining, expected {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_params\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msparsity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstore_mask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quit' is not defined"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
    "\n",
    "model = train.SingleFcNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=28**2,\n",
    "                        out_dim=87,\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = SynFlow(masked_parameters_)\n",
    "\n",
    "compression = 0.1\n",
    "# args.compression = 10**(arg.compression)\n",
    "sparsity = 10**(-float(compression))\n",
    "print('sparsity is \\t ', str(1 - sparsity))\n",
    "prune_loop(model, nn.CrossEntropyLoss(), pruner, trainLoader, 'cpu', sparsity, schedule='exponential', scope='local', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "            model,\n",
    "            max_lr=1e-3,\n",
    "            lr_mode='step',\n",
    "            step=10,\n",
    "            change_mo=False,\n",
    "            epochs=10,\n",
    "        #     epochs=1,\n",
    "            print_freq=100,\n",
    "            tune_alpha=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Single convolution MON, CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 277.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking checked\n",
      "Globally masking checked\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'rfft'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-05f916227547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m train.train(trainLoader, testLoader,\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mmax_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainLoader, testLoader, model, epochs, max_lr, print_freq, change_mo, model_path, lr_mode, step, tune_alpha, max_alpha)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mce_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0mzs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\splitting.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Run the forward pass _without_ tracking gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             z = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_inverse\u001b[1;34m(self, alpha, beta)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         conv_fft_A = [init_fft_conv(self.A(i), (self.conv_shp[i], self.conv_shp[i]))\n\u001b[0m\u001b[0;32m    795\u001b[0m                       for i in range(n)]\n\u001b[0;32m    796\u001b[0m         conv_fft_B = [init_fft_conv(self.B(i), (self.conv_shp[i], self.conv_shp[i]))\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         conv_fft_A = [init_fft_conv(self.A(i), (self.conv_shp[i], self.conv_shp[i]))\n\u001b[0m\u001b[0;32m    795\u001b[0m                       for i in range(n)]\n\u001b[0;32m    796\u001b[0m         conv_fft_B = [init_fft_conv(self.B(i), (self.conv_shp[i], self.conv_shp[i]))\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_fft_conv\u001b[1;34m(weight, hw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     kernel = F.pad(F.pad(kernel, (0, hw[0] - weight.shape[2], 0, hw[1] - weight.shape[3])),\n\u001b[0;32m    189\u001b[0m                    (0, py, 0, px), mode=\"circular\")[:, :, py:, px:]\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfft_to_complex_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monesided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'rfft'"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=False)\n",
    "\n",
    "model = train.MultiConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        conv_sizes=(16,32,60),\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = Rand(masked_parameters_)\n",
    "\n",
    "compression = 3\n",
    "\n",
    "# args.compression = 10**(arg.compression)\n",
    "sparsity = 10**(-float(compression))\n",
    "prune_loop(model, None, pruner, None, 'cpu', sparsity, schedule='exponential', scope='global', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "            model,\n",
    "        max_lr=1e-2,\n",
    "        lr_mode='step',\n",
    "        step=10,\n",
    "        change_mo=False,\n",
    "#         epochs=40,\n",
    "        epochs=1,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Sparsity: \t  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking checked\n",
      "Globally masking checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'rfft'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0b651508f7c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m train.train(trainLoader, testLoader,\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mmax_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainLoader, testLoader, model, epochs, max_lr, print_freq, change_mo, model_path, lr_mode, step, tune_alpha, max_alpha)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mce_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\train.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0mzs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pvh\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\splitting.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Run the forward pass _without_ tracking gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             z = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_inverse\u001b[1;34m(self, alpha, beta)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         conv_fft_A = [init_fft_conv(self.A(i), (self.conv_shp[i], self.conv_shp[i]))\n\u001b[0m\u001b[0;32m    795\u001b[0m                       for i in range(n)]\n\u001b[0;32m    796\u001b[0m         conv_fft_B = [init_fft_conv(self.B(i), (self.conv_shp[i], self.conv_shp[i]))\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         conv_fft_A = [init_fft_conv(self.A(i), (self.conv_shp[i], self.conv_shp[i]))\n\u001b[0m\u001b[0;32m    795\u001b[0m                       for i in range(n)]\n\u001b[0;32m    796\u001b[0m         conv_fft_B = [init_fft_conv(self.B(i), (self.conv_shp[i], self.conv_shp[i]))\n",
      "\u001b[1;32md:\\FPTAI\\Code\\DEQ\\lth_deq\\mon.py\u001b[0m in \u001b[0;36minit_fft_conv\u001b[1;34m(weight, hw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     kernel = F.pad(F.pad(kernel, (0, hw[0] - weight.shape[2], 0, hw[1] - weight.shape[3])),\n\u001b[0;32m    189\u001b[0m                    (0, py, 0, px), mode=\"circular\")[:, :, py:, px:]\n\u001b[1;32m--> 190\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfft_to_complex_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monesided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'rfft'"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = train.cifar_loaders(train_batch_size=128, test_batch_size=400, augment=False)\n",
    "\n",
    "model = train.MultiConvNet(sp.MONPeacemanRachford,\n",
    "                        in_dim=32,\n",
    "                        in_channels=3,\n",
    "                        conv_sizes=(64,128,128),\n",
    "                        alpha=1.0,\n",
    "                        max_iter=300,\n",
    "                        tol=1e-2,\n",
    "                        m=1.0,\n",
    "                        is_pruning=True)\n",
    "\n",
    "# print('Pruning with {} for {} epochs.'.format(args.pruner, args.prune_epochs))\n",
    "masked_parameters_ = masked_parameters(model)\n",
    "pruner = Rand(masked_parameters_)\n",
    "\n",
    "sparsity = 1 \n",
    "print('Sparsity: \\t ', 1 - sparsity)\n",
    "prune_loop(model, None, pruner, None, 'cpu', sparsity, schedule='exponential', scope='global', epochs=1,\n",
    "               reinitialize=False, train_mode=False, shuffle=False, invert=False,)\n",
    "\n",
    "##### Remove A or B ##### \n",
    "with torch.no_grad():\n",
    "    # model.mon.linear_module.A.weight_mask = torch.zeros_like(model.mon.linear_module.A.weight_mask) + torch.eye(model.mon.linear_module.A.weight_mask.shape[0])\n",
    "    # model.mon.linear_module.B.weight_mask = torch.zeros_like(model.mon.linear_module.B.weight_mask) + torch.eye(model.mon.linear_module.B.weight_mask.shape[0])\n",
    "    # model.mon.linear_module.A.weight_mask = torch.zeros_like(model.mon.linear_module.A.weight_mask)\n",
    "    # model.mon.linear_module.B.weight_mask = torch.zeros_like(model.mon.linear_module.B.weight_mask)\n",
    "    for n, m in model.named_buffers():\n",
    "        if 'A' in n:\n",
    "            m.copy_(torch.zeros_like(m))\n",
    "\n",
    "\n",
    "train.train(trainLoader, testLoader,\n",
    "        model,\n",
    "        max_lr=0.05,\n",
    "        lr_mode='1cycle',\n",
    "        change_mo=True,\n",
    "#         epochs=65,\n",
    "        epochs=10,\n",
    "        print_freq=100,\n",
    "        tune_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
