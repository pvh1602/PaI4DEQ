{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMm8yOtbHZC9"
      },
      "source": [
        "# lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D63Mw_Sf6Xw",
        "outputId": "689acb29-096f-4372-de76-db6de3e4530e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.4 in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision==0.5 in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy==1.18 in /usr/local/lib/python3.7/dist-packages (1.18.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==1.4 torchvision==0.5 numpy==1.18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "67VDJUoa6kLz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "#import mon\n",
        "import numpy as np\n",
        "import time\n",
        "#\n",
        "def cuda(tensor):\n",
        "    if torch.cuda.is_available():\n",
        "        return tensor.cuda()\n",
        "    else:\n",
        "        return tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pruner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pruner:\n",
        "    def __init__(self, masked_parameters):\n",
        "        self.masked_parameters = list(masked_parameters)\n",
        "        self.scores = {}\n",
        "\n",
        "    def score(self, model, loss, dataloader, device):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _global_mask(self, sparsity):\n",
        "        r\"\"\"Updates masks of model with scores by sparsity level globally.\n",
        "        \"\"\"\n",
        "        # # Set score for masked parameters to -inf \n",
        "        # for mask, param in self.masked_parameters:\n",
        "        #     score = self.scores[id(param)]\n",
        "        #     score[mask == 0.0] = -np.inf\n",
        "\n",
        "        # Threshold scores\n",
        "        global_scores = torch.cat([torch.flatten(v) for v in self.scores.values()])\n",
        "        k = int((1.0 - sparsity) * global_scores.numel())\n",
        "        if not k < 1:\n",
        "            threshold, _ = torch.kthvalue(global_scores, k)\n",
        "            for mask, param in self.masked_parameters:\n",
        "                score = self.scores[id(param)] \n",
        "                zero = torch.tensor([0.]).to(mask.device)\n",
        "                one = torch.tensor([1.]).to(mask.device)\n",
        "                mask.copy_(torch.where(score <= threshold, zero, one))\n",
        "    \n",
        "    def _local_mask(self, sparsity):\n",
        "        r\"\"\"Updates masks of model with scores by sparsity level parameter-wise.\n",
        "        \"\"\"\n",
        "        for mask, param in self.masked_parameters:\n",
        "            score = self.scores[id(param)]\n",
        "            k = int((1.0 - sparsity) * score.numel())\n",
        "            if not k < 1:\n",
        "                threshold, _ = torch.kthvalue(torch.flatten(score), k)\n",
        "                zero = torch.tensor([0.]).to(mask.device)\n",
        "                one = torch.tensor([1.]).to(mask.device)\n",
        "                mask.copy_(torch.where(score <= threshold, zero, one))\n",
        "\n",
        "    def mask(self, sparsity, scope):\n",
        "        r\"\"\"Updates masks of model with scores by sparsity according to scope.\n",
        "        \"\"\"\n",
        "        if scope == 'global':\n",
        "            self._global_mask(sparsity)\n",
        "        if scope == 'local':\n",
        "            self._local_mask(sparsity)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_mask(self):\n",
        "        r\"\"\"Applies mask to prunable parameters.\n",
        "        \"\"\"\n",
        "        for mask, param in self.masked_parameters:\n",
        "            param.mul_(mask)\n",
        "\n",
        "    def alpha_mask(self, alpha):\n",
        "        r\"\"\"Set all masks to alpha in model.\n",
        "        \"\"\"\n",
        "        for mask, _ in self.masked_parameters:\n",
        "            mask.fill_(alpha)\n",
        "\n",
        "    # Based on https://github.com/facebookresearch/open_lth/blob/master/utils/tensor_utils.py#L43\n",
        "    def shuffle(self):\n",
        "        for mask, param in self.masked_parameters:\n",
        "            shape = mask.shape\n",
        "            perm = torch.randperm(mask.nelement())\n",
        "            mask = mask.reshape(-1)[perm].reshape(shape)\n",
        "\n",
        "    def invert(self):\n",
        "        for v in self.scores.values():\n",
        "            v.div_(v**2)\n",
        "\n",
        "    def stats(self):\n",
        "        r\"\"\"Returns remaining and total number of prunable parameters.\n",
        "        \"\"\"\n",
        "        remaining_params, total_params = 0, 0 \n",
        "        for mask, _ in self.masked_parameters:\n",
        "             remaining_params += mask.detach().cpu().numpy().sum()\n",
        "             total_params += mask.numel()\n",
        "        return remaining_params, total_params\n",
        "\n",
        "\n",
        "class Rand(Pruner):\n",
        "    def __init__(self, masked_parameters):\n",
        "        super(Rand, self).__init__(masked_parameters)\n",
        "\n",
        "    def score(self, model, loss, dataloader, device):\n",
        "        for _, p in self.masked_parameters:\n",
        "            self.scores[id(p)] = torch.randn_like(p)\n",
        "\n",
        "\n",
        "class Mag(Pruner):\n",
        "    def __init__(self, masked_parameters):\n",
        "        super(Mag, self).__init__(masked_parameters)\n",
        "    \n",
        "    def score(self, model, loss, dataloader, device):\n",
        "        for _, p in self.masked_parameters:\n",
        "            self.scores[id(p)] = torch.clone(p.data).detach().abs_()\n",
        "\n",
        "\n",
        "# Based on https://github.com/mi-lad/snip/blob/master/snip.py#L18\n",
        "class SNIP(Pruner):\n",
        "    def __init__(self, masked_parameters):\n",
        "        super(SNIP, self).__init__(masked_parameters)\n",
        "\n",
        "    def score(self, model, loss, dataloader, device):\n",
        "\n",
        "        # allow masks to have gradient\n",
        "        for m, _ in self.masked_parameters:\n",
        "            m.requires_grad = True\n",
        "\n",
        "        # compute gradient\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss(output, target).backward()\n",
        "\n",
        "        # calculate score |g * theta|\n",
        "        for m, p in self.masked_parameters:\n",
        "            self.scores[id(p)] = torch.clone(m.grad).detach().abs_()\n",
        "            p.grad.data.zero_()\n",
        "            m.grad.data.zero_()\n",
        "            m.requires_grad = False\n",
        "        \n",
        "        for i, v in enumerate(self.scores.values()):\n",
        "            print(f'norm of layer {i+1} is {torch.norm(v)}')\n",
        "\n",
        "        # normalize score\n",
        "        all_scores = torch.cat([torch.flatten(v) for v in self.scores.values()])\n",
        "        norm = torch.sum(all_scores)\n",
        "        for _, p in self.masked_parameters:\n",
        "            self.scores[id(p)].div_(norm)\n",
        "\n",
        "\n",
        "# Based on https://github.com/alecwangcq/GraSP/blob/master/pruner/GraSP.py#L49\n",
        "class GraSP(Pruner):\n",
        "    def __init__(self, masked_parameters):\n",
        "        super(GraSP, self).__init__(masked_parameters)\n",
        "        self.temp = 200\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def score(self, model, loss, dataloader, device):\n",
        "\n",
        "        # first gradient vector without computational graph\n",
        "        stopped_grads = 0\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data) / self.temp\n",
        "            L = loss(output, target)\n",
        "\n",
        "            grads = torch.autograd.grad(L, [p for (_, p) in self.masked_parameters], create_graph=False)\n",
        "            flatten_grads = torch.cat([g.reshape(-1) for g in grads if g is not None])\n",
        "            stopped_grads += flatten_grads\n",
        "\n",
        "        # second gradient vector with computational graph\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data) / self.temp\n",
        "            L = loss(output, target)\n",
        "\n",
        "            grads = torch.autograd.grad(L, [p for (_, p) in self.masked_parameters], create_graph=True)\n",
        "            flatten_grads = torch.cat([g.reshape(-1) for g in grads if g is not None])\n",
        "            \n",
        "            gnorm = (stopped_grads * flatten_grads).sum()\n",
        "            gnorm.backward()\n",
        "        \n",
        "        # calculate score Hg * theta (negate to remove top percent)\n",
        "        for _, p in self.masked_parameters:\n",
        "            self.scores[id(p)] = torch.clone(p.grad * p.data).detach()\n",
        "            p.grad.data.zero_()\n",
        "\n",
        "        # normalize score\n",
        "        all_scores = torch.cat([torch.flatten(v) for v in self.scores.values()])\n",
        "        norm = torch.abs(torch.sum(all_scores)) + self.eps\n",
        "        for _, p in self.masked_parameters:\n",
        "            self.scores[id(p)].div_(norm)\n",
        "\n",
        "\n",
        "class SynFlow(Pruner):\n",
        "    def __init__(self, masked_parameters):\n",
        "        super(SynFlow, self).__init__(masked_parameters)\n",
        "\n",
        "    def score(self, model, loss, dataloader, device):\n",
        "      \n",
        "        @torch.no_grad()\n",
        "        def linearize(model):\n",
        "            # model.double()\n",
        "            signs = {}\n",
        "            for name, param in model.state_dict().items():\n",
        "                signs[name] = torch.sign(param)\n",
        "                param.abs_()\n",
        "            return signs\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def nonlinearize(model, signs):\n",
        "            # model.float()\n",
        "            for name, param in model.state_dict().items():\n",
        "                param.mul_(signs[name])\n",
        "        \n",
        "        signs = linearize(model)\n",
        "\n",
        "        (data, _) = next(iter(dataloader))\n",
        "        input_dim = list(data[0,:].shape)\n",
        "        input = torch.ones([1] + input_dim).to(device)#, dtype=torch.float64).to(device)\n",
        "        output = model(input)\n",
        "        torch.sum(output).backward()\n",
        "        \n",
        "        for _, p in self.masked_parameters:\n",
        "            self.scores[id(p)] = torch.clone(p.grad * p).detach().abs_()\n",
        "            p.grad.data.zero_()\n",
        "\n",
        "        nonlinearize(model, signs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2783,  0.1714, -0.4783,  0.0742,  0.0288,  0.4450,  0.1079,  0.4279,\n",
              "         -0.1227, -0.3381],\n",
              "        [ 0.2989, -0.0272, -0.0189,  0.2187,  0.0531,  0.3550, -0.2004,  0.2271,\n",
              "         -0.4213,  0.2937],\n",
              "        [-0.2428,  0.1546,  0.3518,  0.4355,  0.2819, -0.1301,  0.1143,  0.0624,\n",
              "         -0.3870,  0.3932],\n",
              "        [-0.3901, -0.4208,  0.2806, -0.0792, -0.1617, -0.2824, -0.2249,  0.2954,\n",
              "         -0.0652, -0.4695],\n",
              "        [-0.3162,  0.4881,  0.1006, -0.4630,  0.1849, -0.0221, -0.3098,  0.4377,\n",
              "          0.3013,  0.1811],\n",
              "        [ 0.2289, -0.4438,  0.4827,  0.4618, -0.2580, -0.4855, -0.1030,  0.3854,\n",
              "          0.3921, -0.0597],\n",
              "        [ 0.4564, -0.4202, -0.3125, -0.4881,  0.0912,  0.0602, -0.3711,  0.1108,\n",
              "         -0.4060, -0.0427],\n",
              "        [-0.3145, -0.1864,  0.4832, -0.0598, -0.3233, -0.2183, -0.2146,  0.0414,\n",
              "         -0.4638, -0.4166],\n",
              "        [-0.1902,  0.3340, -0.2034, -0.1202,  0.2628,  0.1837, -0.2106,  0.2514,\n",
              "          0.1065,  0.0827],\n",
              "        [ 0.0393, -0.1975, -0.3588, -0.1591,  0.2398,  0.4583,  0.0707,  0.0780,\n",
              "         -0.2876, -0.1741]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "init_mask_weight = torch.rand(10,10)-0.5\n",
        "init_mask_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(55)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "init_mask_weight = torch.rand(10,10)-0.5\n",
        "init_mask_weight = torch.where(init_mask_weight > 0, 1, 0) \n",
        "init_mask_weight.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl7RAsStBcrE"
      },
      "source": [
        "# mon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Linear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(Linear, self).__init__(in_features, out_features, bias)      \n",
        "        init_mask_weight = torch.rand(self.weight.shape) - 0.5\n",
        "        init_mask_weight = torch.where(init_mask_weight > 0, 1, 0)  \n",
        "        self.register_buffer('weight_mask', init_mask_weight)\n",
        "        if self.bias is not None:    \n",
        "            init_mask_bias = torch.rand(self.bias.shape) - 0.5\n",
        "            init_mask_bias = torch.where(init_mask_bias > 0, 1, 0)  \n",
        "            self.register_buffer('bias_mask', init_mask_bias)\n",
        "\n",
        "    def get_masked_weight(self):\n",
        "        W = self.weight_mask * self.weight\n",
        "        return W\n",
        "\n",
        "    def forward(self, input):\n",
        "        W = self.weight_mask * self.weight\n",
        "        if self.bias is not None:\n",
        "            b = self.bias_mask * self.bias\n",
        "        else:\n",
        "            b = self.bias\n",
        "        return F.linear(input, W, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5VJISTPeCqMg"
      },
      "outputs": [],
      "source": [
        "class MONSingleFc(nn.Module):\n",
        "    \"\"\" Simple MON linear class, just a single full multiply. \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, m=1.0):\n",
        "        super().__init__()\n",
        "        self.U = nn.Linear(in_dim, out_dim)\n",
        "        self.A = nn.Linear(out_dim, out_dim, bias=False)\n",
        "        self.B = nn.Linear(out_dim, out_dim, bias=False)\n",
        "        self.m = m\n",
        "        self.dropout = nn.Dropout(p=0.8)\n",
        "\n",
        "    def x_shape(self, n_batch):\n",
        "        return (n_batch, self.U.in_features)\n",
        "\n",
        "    def z_shape(self, n_batch):\n",
        "        return ((n_batch, self.A.in_features),)\n",
        "\n",
        "    def forward(self, x, *z):\n",
        "        out = (self.dropout(self.U(x) + self.multiply(*z)[0]),)\n",
        "        return out\n",
        "\n",
        "    def bias(self, x):\n",
        "        return (self.U(x),)\n",
        "\n",
        "    def multiply(self, *z):\n",
        "        ATAz = self.A(z[0]) @ self.A.weight\n",
        "        z_out = (1 - self.m) * z[0] - ATAz + self.B(z[0]) - z[0] @ self.B.weight\n",
        "        return (z_out,)\n",
        "\n",
        "    def multiply_transpose(self, *g):\n",
        "        ATAg = self.A(g[0]) @ self.A.weight\n",
        "        g_out = (1 - self.m) * g[0] - ATAg - self.B(g[0]) + g[0] @ self.B.weight\n",
        "        return (g_out,)\n",
        "\n",
        "    def init_inverse(self, alpha, beta):\n",
        "        I = torch.eye(self.A.weight.shape[0], dtype=self.A.weight.dtype,\n",
        "                      device=self.A.weight.device)\n",
        "        W = (1 - self.m) * I - self.A.weight.T @ self.A.weight + self.B.weight - self.B.weight.T\n",
        "        self.Winv = torch.inverse(alpha * I + beta * W)\n",
        "\n",
        "    def inverse(self, *z):\n",
        "        return (z[0] @ self.Winv.transpose(0, 1),)\n",
        "\n",
        "    def inverse_transpose(self, *g):\n",
        "        return (g[0] @ self.Winv,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskedMONSingleFc(nn.Module):\n",
        "    \"\"\" Simple MON linear class, just a single full multiply. \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, m=1.0):\n",
        "        super().__init__()\n",
        "        self.U = Linear(in_dim, out_dim)\n",
        "        self.A = Linear(out_dim, out_dim, bias=False)\n",
        "        self.B = Linear(out_dim, out_dim, bias=False)\n",
        "        self.m = m\n",
        "        # self.dropout = nn.Dropout(p=0.8)\n",
        "\n",
        "    def x_shape(self, n_batch):\n",
        "        return (n_batch, self.U.in_features)\n",
        "\n",
        "    def z_shape(self, n_batch):\n",
        "        return ((n_batch, self.A.in_features),)\n",
        "\n",
        "    def forward(self, x, *z):\n",
        "        out = (self.U(x) + self.multiply(*z)[0],)\n",
        "        return out\n",
        "\n",
        "    def bias(self, x):\n",
        "        return (self.U(x),)\n",
        "\n",
        "    def multiply(self, *z):\n",
        "        ATAz = self.A(z[0]) @ self.A.get_masked_weight()\n",
        "        z_out = (1 - self.m) * z[0] - ATAz + self.B(z[0]) - z[0] @ self.B.get_masked_weight()\n",
        "        return (z_out,)\n",
        "\n",
        "    def multiply_transpose(self, *g):\n",
        "        ATAg = self.A(g[0]) @ self.A.get_masked_weight()\n",
        "        g_out = (1 - self.m) * g[0] - ATAg - self.B(g[0]) + g[0] @ self.B.get_masked_weight()\n",
        "        return (g_out,)\n",
        "\n",
        "    def init_inverse(self, alpha, beta):\n",
        "        I = torch.eye(self.A.get_masked_weight().shape[0], dtype=self.A.get_masked_weight().dtype,\n",
        "                      device=self.A.get_masked_weight().device)\n",
        "        W = (1 - self.m) * I - self.A.get_masked_weight().T @ self.A.get_masked_weight() + self.B.get_masked_weight() - self.B.get_masked_weight().T\n",
        "        self.Winv = torch.inverse(alpha * I + beta * W)\n",
        "\n",
        "    def inverse(self, *z):\n",
        "        return (z[0] @ self.Winv.transpose(0, 1),)\n",
        "\n",
        "    def inverse_transpose(self, *g):\n",
        "        return (g[0] @ self.Winv,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iG16LFvcCqP5"
      },
      "outputs": [],
      "source": [
        "class MONReLU(nn.Module):\n",
        "    def forward(self, *z):\n",
        "        return tuple(F.relu(z_) for z_ in z)\n",
        "\n",
        "    def derivative(self, *z):\n",
        "        return tuple((z_ > 0).type_as(z[0]) for z_ in z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Y3Mf3SBqd_"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PPe2hu6j-GLR"
      },
      "outputs": [],
      "source": [
        "class Meter(object):\n",
        "    \"\"\"Computes and stores the min, max, avg, and current values\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.max = -float(\"inf\")\n",
        "        self.min = float(\"inf\")\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        self.max = max(self.max, val)\n",
        "        self.min = min(self.min, val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y9X_CfWI-Pnw"
      },
      "outputs": [],
      "source": [
        "class SplittingMethodStats(object):\n",
        "    def __init__(self):\n",
        "        self.fwd_iters = Meter()\n",
        "        self.bkwd_iters = Meter()\n",
        "        self.fwd_time = Meter()\n",
        "        self.bkwd_time = Meter()\n",
        "\n",
        "    def reset(self):\n",
        "        self.fwd_iters.reset()\n",
        "        self.fwd_time.reset()\n",
        "        self.bkwd_iters.reset()\n",
        "        self.bkwd_time.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ftb2Pi3E1CY"
      },
      "source": [
        "# Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GO1er_Pl-_sM"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "37_wyDO0_eAc"
      },
      "outputs": [],
      "source": [
        "class MONForwardBackwardSplitting(nn.Module):\n",
        "\n",
        "    def __init__(self, linear_module, nonlin_module, alpha=1.0, tol=1e-5, max_iter=50, verbose=False):\n",
        "        super().__init__()\n",
        "        self.linear_module = linear_module\n",
        "        self.nonlin_module = nonlin_module\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.stats = SplittingMethodStats()\n",
        "        self.save_abs_err = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass of the MON, find an equilibirum with forward-backward splitting\"\"\"\n",
        "\n",
        "        start = time.time()\n",
        "        # Run the forward pass _without_ tracking gradients\n",
        "        with torch.no_grad():\n",
        "            z = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
        "                      for s in self.linear_module.z_shape(x.shape[0]))\n",
        "            n = len(z)\n",
        "            bias = self.linear_module.bias(x)\n",
        "\n",
        "            err = 1.0\n",
        "            it = 0\n",
        "            errs = []\n",
        "            while (err > self.tol and it < self.max_iter):\n",
        "                zn = self.linear_module.multiply(*z)\n",
        "                zn = tuple((1 - self.alpha) * z[i] + self.alpha * (zn[i] + bias[i]) for i in range(n))\n",
        "                zn = self.nonlin_module(*zn)\n",
        "                if self.save_abs_err:\n",
        "                    fn = self.nonlin_module(*self.linear_module(x, *zn))\n",
        "                    err = sum((zn[i] - fn[i]).norm().item() / (zn[i].norm().item()) for i in range(n))\n",
        "                    errs.append(err)\n",
        "                else:\n",
        "                    err = sum((zn[i] - z[i]).norm().item() / (1e-6 + zn[i].norm().item()) for i in range(n))\n",
        "                z = zn\n",
        "                it = it + 1\n",
        "\n",
        "        # Run the forward pass one more time, tracking gradients, then backward placeholder\n",
        "        zn = self.linear_module(x, *z)\n",
        "        zn = self.nonlin_module(*zn)\n",
        "        zn = self.Backward.apply(self, *zn)\n",
        "        self.stats.fwd_iters.update(it)\n",
        "        self.stats.fwd_time.update(time.time() - start)\n",
        "        self.errs = errs\n",
        "        return zn\n",
        "\n",
        "    class Backward(Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, splitter, *z):\n",
        "            ctx.splitter = splitter\n",
        "            ctx.save_for_backward(*z)\n",
        "            return z\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, *g):\n",
        "            start = time.time()\n",
        "            sp = ctx.splitter\n",
        "            n = len(g)\n",
        "            z = ctx.saved_tensors\n",
        "            j = sp.nonlin_module.derivative(*z)\n",
        "            I = [j[i] == 0 for i in range(n)]\n",
        "            d = [(1 - j[i]) / j[i] for i in range(n)]\n",
        "            v = tuple(j[i] * g[i] for i in range(n))\n",
        "            u = tuple(torch.zeros(s, dtype=g[0].dtype, device=g[0].device)\n",
        "                      for s in sp.linear_module.z_shape(g[0].shape[0]))\n",
        "\n",
        "            err = 1.0\n",
        "            it = 0\n",
        "            errs = []\n",
        "            while (err > sp.tol and it < sp.max_iter):\n",
        "                un = sp.linear_module.multiply_transpose(*u)\n",
        "                un = tuple((1 - sp.alpha) * u[i] + sp.alpha * un[i] for i in range(n))\n",
        "                un = tuple((un[i] + sp.alpha * (1 + d[i]) * v[i]) / (1 + sp.alpha * d[i]) for i in range(n))\n",
        "                for i in range(n):\n",
        "                    un[i][I[i]] = v[i][I[i]]\n",
        "\n",
        "                err = sum((un[i] - u[i]).norm().item() / (1e-6 + un[i].norm().item()) for i in range(n))\n",
        "                errs.append(err)\n",
        "                u = un\n",
        "                it = it + 1\n",
        "\n",
        "            dg = sp.linear_module.multiply_transpose(*u)\n",
        "            dg = tuple(g[i] + dg[i] for i in range(n))\n",
        "\n",
        "            sp.stats.bkwd_iters.update(it)\n",
        "            sp.stats.bkwd_time.update(time.time() - start)\n",
        "            sp.errs = errs\n",
        "            return (None,) + dg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e4sdxO-6_G0w"
      },
      "outputs": [],
      "source": [
        "class MONPeacemanRachford(nn.Module):\n",
        "\n",
        "    def __init__(self, linear_module, nonlin_module, alpha=1.0, tol=1e-5, max_iter=50, verbose=False):\n",
        "        super().__init__()\n",
        "        self.linear_module = linear_module\n",
        "        self.nonlin_module = nonlin_module\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "        self.stats = SplittingMethodStats()\n",
        "        self.save_abs_err = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass of the MON, find an equilibirum with forward-backward splitting\"\"\"\n",
        "\n",
        "        start = time.time()\n",
        "        # Run the forward pass _without_ tracking gradients\n",
        "        self.linear_module.init_inverse(1 + self.alpha, -self.alpha)\n",
        "        with torch.no_grad():\n",
        "            z = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
        "                      for s in self.linear_module.z_shape(x.shape[0]))\n",
        "            u = tuple(torch.zeros(s, dtype=x.dtype, device=x.device)\n",
        "                      for s in self.linear_module.z_shape(x.shape[0]))\n",
        "\n",
        "            n = len(z)\n",
        "            bias = self.linear_module.bias(x)\n",
        "\n",
        "            err = 1.0\n",
        "            it = 0\n",
        "            errs = []\n",
        "            while (err > self.tol and it < self.max_iter):\n",
        "                u_12 = tuple(2 * z[i] - u[i] for i in range(n))\n",
        "                z_12 = self.linear_module.inverse(*tuple(u_12[i] + self.alpha * bias[i] for i in range(n)))\n",
        "                u = tuple(2 * z_12[i] - u_12[i] for i in range(n))\n",
        "                zn = self.nonlin_module(*u)\n",
        "\n",
        "                if self.save_abs_err:\n",
        "                    fn = self.nonlin_module(*self.linear_module(x, *zn))\n",
        "                    err = sum((zn[i] - fn[i]).norm().item() / (zn[i].norm().item()) for i in range(n))\n",
        "                    errs.append(err)\n",
        "                else:\n",
        "                    err = sum((zn[i] - z[i]).norm().item() / (1e-6 + zn[i].norm().item()) for i in range(n))\n",
        "                z = zn\n",
        "                it = it + 1\n",
        "\n",
        "        zn = self.linear_module(x, *z)\n",
        "        zn = self.nonlin_module(*zn)\n",
        "\n",
        "        zn = self.Backward.apply(self, *zn)\n",
        "        self.stats.fwd_iters.update(it)\n",
        "        self.stats.fwd_time.update(time.time() - start)\n",
        "        self.errs = errs\n",
        "        return zn\n",
        "\n",
        "    class Backward(Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, splitter, *z):\n",
        "            ctx.splitter = splitter\n",
        "            ctx.save_for_backward(*z)\n",
        "            return z\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, *g):\n",
        "            start = time.time()\n",
        "            sp = ctx.splitter\n",
        "            n = len(g)\n",
        "            z = ctx.saved_tensors\n",
        "            j = sp.nonlin_module.derivative(*z)\n",
        "            I = [j[i] == 0 for i in range(n)]\n",
        "            d = [(1 - j[i]) / j[i] for i in range(n)]\n",
        "            v = tuple(j[i] * g[i] for i in range(n))\n",
        "\n",
        "            z = tuple(torch.zeros(s, dtype=g[0].dtype, device=g[0].device)\n",
        "                      for s in sp.linear_module.z_shape(g[0].shape[0]))\n",
        "            u = tuple(torch.zeros(s, dtype=g[0].dtype, device=g[0].device)\n",
        "                      for s in sp.linear_module.z_shape(g[0].shape[0]))\n",
        "\n",
        "            err = 1.0\n",
        "            errs=[]\n",
        "            it = 0\n",
        "            while (err >sp.tol and it < sp.max_iter):\n",
        "                u_12 = tuple(2 * z[i] - u[i] for i in range(n))\n",
        "                z_12 = sp.linear_module.inverse_transpose(*u_12)\n",
        "                u = tuple(2 * z_12[i] - u_12[i] for i in range(n))\n",
        "                zn = tuple((u[i] + sp.alpha * (1 + d[i]) * v[i]) / (1 + sp.alpha * d[i]) for i in range(n))\n",
        "                for i in range(n):\n",
        "                    zn[i][I[i]] = v[i][I[i]]\n",
        "\n",
        "                err = sum((zn[i] - z[i]).norm().item() / (1e-6 + zn[i].norm().item()) for i in range(n))\n",
        "                errs.append(err)\n",
        "                z = zn\n",
        "                it = it + 1\n",
        "\n",
        "            dg = sp.linear_module.multiply_transpose(*zn)\n",
        "            dg = tuple(g[i] + dg[i] for i in range(n))\n",
        "\n",
        "            sp.stats.bkwd_iters.update(it)\n",
        "            sp.stats.bkwd_time.update(time.time() - start)\n",
        "            sp.errs = errs\n",
        "            return (None,) + dg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp_2kplcGnXC"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-mbq3ImG7mOZ"
      },
      "outputs": [],
      "source": [
        "def train(trainLoader, testLoader, model, epochs=15, max_lr=1e-3,\n",
        "          print_freq=10, change_mo=True, model_path=None, lr_mode='step',\n",
        "          step=10,tune_alpha=False,max_alpha=1.):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=max_lr)\n",
        "\n",
        "    if lr_mode == '1cycle':\n",
        "        lr_schedule = lambda t: np.interp([t],\n",
        "                                          [0, (epochs-5)//2, epochs-5, epochs],\n",
        "                                          [1e-3, max_lr, 1e-3, 1e-3])[0]\n",
        "    elif lr_mode == 'step':\n",
        "        lr_scheduler =optim.lr_scheduler.StepLR(optimizer, step, gamma=0.1, last_epoch=-1)\n",
        "    elif lr_mode != 'constant':\n",
        "        raise Exception('lr mode one of constant, step, 1cycle')\n",
        "\n",
        "    if change_mo:\n",
        "        max_mo = 0.85\n",
        "        momentum_schedule = lambda t: np.interp([t],\n",
        "                                                [0, (epochs - 5) // 2, epochs - 5, epochs],\n",
        "                                                [0.95, max_mo, 0.95, 0.95])[0]\n",
        "\n",
        "    model = cuda(model)\n",
        "\n",
        "    for epoch in range(1, 1 + epochs):\n",
        "        nProcessed = 0\n",
        "        nTrain = len(trainLoader.dataset)\n",
        "        model.train()\n",
        "        start = time.time()\n",
        "        for batch_idx, batch in enumerate(trainLoader):\n",
        "            if (batch_idx  == 30 or batch_idx == int(len(trainLoader)/2)) and tune_alpha:\n",
        "                run_tune_alpha(model, cuda(batch[0]), max_alpha)\n",
        "            if lr_mode == '1cycle':\n",
        "                lr = lr_schedule(epoch -  1 + batch_idx/ len(trainLoader))\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr\n",
        "            if change_mo:\n",
        "                beta1 = momentum_schedule(epoch - 1 + batch_idx / len(trainLoader))\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['betas'] = (beta1, optimizer.param_groups[0]['betas'][1])\n",
        "\n",
        "            data, target = cuda(batch[0]), cuda(batch[1])\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(data)\n",
        "            ce_loss = nn.CrossEntropyLoss()(preds, target)\n",
        "            ce_loss.backward()\n",
        "            nProcessed += len(data)\n",
        "            if batch_idx % print_freq == 0 and batch_idx > 0:\n",
        "                incorrect = preds.float().argmax(1).ne(target.data).sum()\n",
        "                err = 100. * incorrect.float() / float(len(data))\n",
        "                partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        if lr_mode == 'step':\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        if model_path is not None:\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        start = time.time()\n",
        "        test_loss = 0\n",
        "        incorrect = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in testLoader:\n",
        "                data, target = cuda(batch[0]), cuda(batch[1])\n",
        "                preds = model(data)\n",
        "                ce_loss = nn.CrossEntropyLoss(reduction='sum')(preds, target)\n",
        "                test_loss += ce_loss\n",
        "                incorrect += preds.float().argmax(1).ne(target.data).sum()\n",
        "            test_loss /= len(testLoader.dataset)\n",
        "            nTotal = len(testLoader.dataset)\n",
        "            err = 100. * incorrect.float() / float(nTotal)\n",
        "            print('\\n\\n\\n Epoch: {:d},Test set: Average loss: {:.4f}, Error: {}/{} ({:.2f}%)'.format(\n",
        "                epoch, test_loss, incorrect, nTotal, err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "USq376Q-78Nx"
      },
      "outputs": [],
      "source": [
        "def run_tune_alpha(model, x, max_alpha):\n",
        "    orig_alpha  =  model.mon.alpha\n",
        "    model.mon.stats.reset()\n",
        "    model.mon.alpha = max_alpha\n",
        "    with torch.no_grad():\n",
        "        model(x)\n",
        "    iters = model.mon.stats.fwd_iters.val\n",
        "    model.mon.stats.reset()\n",
        "    iters_n = iters\n",
        "    while model.mon.alpha > 1e-4 and iters_n <= iters:\n",
        "        model.mon.alpha = model.mon.alpha/2\n",
        "        with torch.no_grad():\n",
        "            model(x)\n",
        "        iters = iters_n\n",
        "        iters_n = model.mon.stats.fwd_iters.val\n",
        "        model.mon.stats.reset()\n",
        "\n",
        "    if iters==model.mon.max_iter:\n",
        "        print(\"none converged, resetting to current\")\n",
        "        model.mon.alpha=orig_alpha\n",
        "    else:\n",
        "        model.mon.alpha = model.mon.alpha * 2\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y0dLFanl8G0J"
      },
      "outputs": [],
      "source": [
        "def mnist_loaders(train_batch_size, test_batch_size=None):\n",
        "    if test_batch_size is None:\n",
        "        test_batch_size = train_batch_size\n",
        "\n",
        "    trainLoader = torch.utils.data.DataLoader(\n",
        "        dset.MNIST('data',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "        batch_size=train_batch_size,\n",
        "        shuffle=True)\n",
        "\n",
        "    testLoader = torch.utils.data.DataLoader(\n",
        "        dset.MNIST('data',\n",
        "                   train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=False)\n",
        "    return trainLoader, testLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qp-UySHc8l8F"
      },
      "outputs": [],
      "source": [
        "def expand_args(defaults, kwargs):\n",
        "    d = defaults.copy()\n",
        "    for k, v in kwargs.items():\n",
        "        d[k] = v\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UdXei2YN8osh"
      },
      "outputs": [],
      "source": [
        "MON_DEFAULTS = {\n",
        "    'alpha': 1.0,\n",
        "    'tol': 1e-5,\n",
        "    'max_iter': 50\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4Cb5K2FlWZ2i"
      },
      "outputs": [],
      "source": [
        "class SingleFcNet(nn.Module):\n",
        "\n",
        "    def __init__(self, splittingMethod, in_dim=784, out_dim=100, m=0.1, **kwargs):\n",
        "        super().__init__()\n",
        "        linear_module = MaskedMONSingleFc(in_dim, out_dim, m=m)\n",
        "        nonlin_module = MONReLU()\n",
        "        self.mon = splittingMethod(linear_module, nonlin_module, **expand_args(MON_DEFAULTS, kwargs))\n",
        "        self.Wout = nn.Linear(out_dim, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        z = self.mon(x)\n",
        "        return self.Wout(z[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# from copyreg import pickle\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "def prune_loop(model, loss, pruner, dataloader, device, sparsity, schedule, scope, epochs,\n",
        "               reinitialize=False, train_mode=False, shuffle=False, invert=False, \n",
        "               store_mask=False, pruner_name='', compression='', dataset='mnist', args=None):\n",
        "    r\"\"\"Applies score mask loop iteratively to a final sparsity level.\n",
        "    \"\"\"\n",
        "    # Set model to train or eval mode\n",
        "    model.train()\n",
        "    if not train_mode:\n",
        "        model.eval()\n",
        "\n",
        "    # Prune model\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        pruner.score(model, loss, dataloader, device)\n",
        "        if schedule == 'exponential':\n",
        "            sparse = sparsity**((epoch + 1) / epochs)\n",
        "        elif schedule == 'linear':\n",
        "            sparse = 1.0 - (1.0 - sparsity)*((epoch + 1) / epochs)\n",
        "        # Invert scores\n",
        "        if invert:\n",
        "            pruner.invert()\n",
        "        pruner.mask(sparse, scope)\n",
        "    \n",
        "    # Reainitialize weights\n",
        "    if reinitialize:\n",
        "        model._initialize_weights()\n",
        "\n",
        "    # Shuffle masks\n",
        "    if shuffle:\n",
        "        pruner.shuffle()\n",
        "\n",
        "    # Confirm sparsity level\n",
        "    remaining_params, total_params = pruner.stats()\n",
        "    if np.abs(remaining_params - total_params*sparsity) >= 5:\n",
        "        print(\"ERROR: {} prunable parameters remaining, expected {}\".format(remaining_params, total_params*sparsity))\n",
        "        quit()\n",
        "    \n",
        "    if store_mask:\n",
        "        # pruner_name = args.pruner\n",
        "        if args.shuffle:\n",
        "            pruner_name = 'shuffled_' + args.pruner\n",
        "        if args.pruner in ['grasp', 'snip'] and args.prune_epochs == 100:\n",
        "            pruner_name = 'iterative_' + pruner_name\n",
        "        if args.pruner == 'synflow' and args.prune_epochs == 1:\n",
        "            pruner_name = 'oneshot_' + pruner_name\n",
        "        \n",
        "        if args.model is not 'fc':\n",
        "            file_path = f'./Reproduced_Results/Masks/{args.dataset}_{args.model}/{args.init_type}/pre_epoch_{args.pre_epochs}/compression_{int(args.compression*100)}/{pruner_name}'\n",
        "        else:\n",
        "            file_path = f'./Reproduced_Results/Masks/{args.dataset}_{args.model}/{args.init_type}/pre_epoch_{args.pre_epochs}/MLP_{args.n_layers}_layers_{args.n_neurons}/compression_{int(args.compression*100)}/{pruner_name}'\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            os.makedirs(file_path)\n",
        "        file_name = f'{file_path}/{pruner_name}_{int(compression*100)}.pkl'\n",
        "        stored_data = {}\n",
        "        stored_masks = []\n",
        "        stored_params = []\n",
        "        for m, p in pruner.masked_parameters:\n",
        "            stored_masks.append(m.detach().cpu().numpy())\n",
        "            stored_params.append(p.detach().cpu().numpy())\n",
        "        stored_data['mask'] = stored_masks\n",
        "        stored_data['param'] = stored_params\n",
        "        \n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(stored_data, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDjISqryGurA"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5olR2Rlq6sx_",
        "outputId": "ac363b84-0e35-46b6-d778-1833a9c4ae54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " Epoch: 1,Test set: Average loss: 0.3020, Error: 866/10000 (8.66%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 2,Test set: Average loss: 0.2534, Error: 743/10000 (7.43%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 3,Test set: Average loss: 0.2309, Error: 671/10000 (6.71%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 4,Test set: Average loss: 0.2085, Error: 615/10000 (6.15%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 5,Test set: Average loss: 0.1931, Error: 542/10000 (5.42%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 6,Test set: Average loss: 0.1844, Error: 534/10000 (5.34%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 7,Test set: Average loss: 0.1820, Error: 542/10000 (5.42%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 8,Test set: Average loss: 0.1724, Error: 502/10000 (5.02%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 9,Test set: Average loss: 0.1706, Error: 489/10000 (4.89%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 10,Test set: Average loss: 0.1589, Error: 479/10000 (4.79%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 11,Test set: Average loss: 0.1541, Error: 452/10000 (4.52%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 12,Test set: Average loss: 0.1540, Error: 447/10000 (4.47%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 13,Test set: Average loss: 0.1525, Error: 438/10000 (4.38%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 14,Test set: Average loss: 0.1528, Error: 440/10000 (4.40%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 15,Test set: Average loss: 0.1516, Error: 449/10000 (4.49%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 16,Test set: Average loss: 0.1511, Error: 443/10000 (4.43%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 17,Test set: Average loss: 0.1520, Error: 441/10000 (4.41%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 18,Test set: Average loss: 0.1506, Error: 442/10000 (4.42%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 19,Test set: Average loss: 0.1510, Error: 444/10000 (4.44%)\n",
            "\n",
            "\n",
            "\n",
            " Epoch: 20,Test set: Average loss: 0.1502, Error: 447/10000 (4.47%)\n"
          ]
        }
      ],
      "source": [
        "trainLoader, testLoader = mnist_loaders(train_batch_size=128, test_batch_size=400)\n",
        "\n",
        "model = SingleFcNet(MONPeacemanRachford,\n",
        "                        in_dim=28**2,\n",
        "                        out_dim=20,\n",
        "                        alpha=1.0,\n",
        "                        max_iter=300,\n",
        "                        tol=1e-2,\n",
        "                        m=1.0) #parameter which controls the strong monotonicity of W\n",
        "\n",
        "train(trainLoader, testLoader,\n",
        "        model,\n",
        "        max_lr=1e-3,\n",
        "        lr_mode='step',  #use step decay learning rate\n",
        "        step=10,      \n",
        "        change_mo=False, #do not adjust momentum during training\n",
        "        epochs=20,\n",
        "        print_freq=200,\n",
        "        tune_alpha=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uJFpvr2jra9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(214, device='cuda:0')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.mon.linear_module.A.weight_mask.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([20, 20])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.mon.linear_module.A.weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SingleFcNet(\n",
              "  (mon): MONPeacemanRachford(\n",
              "    (linear_module): MaskedMONSingleFc(\n",
              "      (U): Linear(in_features=784, out_features=20, bias=True)\n",
              "      (A): Linear(in_features=20, out_features=20, bias=False)\n",
              "      (B): Linear(in_features=20, out_features=20, bias=False)\n",
              "    )\n",
              "    (nonlin_module): MONReLU()\n",
              "  )\n",
              "  (Wout): Linear(in_features=20, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "monDEQ-MLP-MNIST",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
